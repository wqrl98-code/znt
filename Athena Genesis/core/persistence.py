# -*- coding: utf-8 -*-
"""
æŒä¹…åŒ–å­˜å‚¨ç®¡ç†å™¨ (Persistence Manager) - ä¼˜åŒ–ä¿®å¤ç‰ˆ
åŠŸèƒ½ï¼š
1. KnowledgeKeeper: æ–‡ä»¶æŒ‡çº¹ä¸åˆ†æç»“æœç¼“å­˜ï¼ˆç§’çº§å¯åŠ¨ï¼‰
2. TaskQueue: SQLiteä»»åŠ¡æŒä¹…åŒ–é˜Ÿåˆ—ï¼ˆå´©æºƒæ¢å¤ + å•ä¾‹æ¨¡å¼ï¼‰
"""
import os
import json
import time
import sqlite3
import uuid
import threading
import logging
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

# é…ç½®æ—¥å¿—
logger = logging.getLogger("PersistenceManager")


# ==========================================
# 1. ä»»åŠ¡é˜Ÿåˆ—æ•°æ®åº“ (å•ä¾‹æ¨¡å¼ï¼Œçº¿ç¨‹å®‰å…¨)
# ==========================================

class TaskQueue:
    """åŸºäºSQLiteçš„ä»»åŠ¡æŒä¹…åŒ–é˜Ÿåˆ—ï¼Œç¡®ä¿ä»»åŠ¡ä¸ä¸¢å¤±"""

    _instance = None
    _lock = threading.Lock()
    _initialized = False

    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼å®ç°ï¼Œç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªä»»åŠ¡é˜Ÿåˆ—å®ä¾‹"""
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(TaskQueue, cls).__new__(cls)
                cls._instance._initialized = False
            return cls._instance

    def __init__(self, db_path: str = None):
        """åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—"""
        if self._initialized:
            return

        if not db_path:
            # é»˜è®¤å­˜åœ¨ Database/tasks.db
            base_dir = os.path.join(os.getcwd(), 'ATHENA_WORKSPACE', 'Database')
            if not os.path.exists(base_dir):
                try:
                    os.makedirs(base_dir, exist_ok=True)
                    logger.info(f"ğŸ“ åˆ›å»ºæ•°æ®åº“ç›®å½•: {base_dir}")
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºæ•°æ®åº“ç›®å½•å¤±è´¥: {e}")
                    raise
            db_path = os.path.join(base_dir, 'tasks.db')

        self.db_path = db_path
        self._init_db()
        self.lock = threading.RLock()
        self._initialized = True
        logger.info(f"âœ… [TaskQueue] åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®åº“è·¯å¾„: {self.db_path}")

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        try:
            with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                cursor = conn.cursor()
                # åˆ›å»ºä»»åŠ¡è¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS tasks (
                        task_id TEXT PRIMARY KEY,
                        task_type TEXT NOT NULL,
                        status TEXT NOT NULL,
                        payload TEXT,
                        result TEXT,
                        created_at REAL,
                        updated_at REAL,
                        retry_count INTEGER DEFAULT 0,
                        error_message TEXT
                    )
                ''')
                # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_status ON tasks(status)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON tasks(created_at)')
                conn.commit()
                logger.info("âœ… [TaskQueue] æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ [TaskQueue] æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def add_task(self, task_type: str, payload: Dict[str, Any]) -> str:
        """æ·»åŠ æ–°ä»»åŠ¡ï¼Œè¿”å› task_id"""
        task_id = str(uuid.uuid4())
        now = time.time()

        try:
            payload_json = json.dumps(payload, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ JSONåºåˆ—åŒ–å¤±è´¥: {e}")
            payload_json = "{}"

        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    conn.execute(
                        "INSERT INTO tasks (task_id, task_type, status, payload, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (task_id, task_type, "PENDING", payload_json, now, now)
                    )
                logger.debug(f"ğŸ“ [TaskQueue] æ·»åŠ ä»»åŠ¡: {task_type} - {task_id}")
            except Exception as e:
                logger.error(f"âŒ å†™å…¥ä»»åŠ¡å¤±è´¥: {e}")
                raise
        return task_id

    def update_status(self, task_id: str, status: str, result: Dict = None, error_message: str = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        now = time.time()
        result_json = None
        if result:
            try:
                result_json = json.dumps(result, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"âš ï¸ ç»“æœåºåˆ—åŒ–å¤±è´¥: {e}")
                result_json = json.dumps({"error": "ç»“æœåºåˆ—åŒ–å¤±è´¥"})

        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    if result_json:
                        conn.execute(
                            "UPDATE tasks SET status = ?, result = ?, updated_at = ?, error_message = ? WHERE task_id = ?",
                            (status, result_json, now, error_message, task_id)
                        )
                    else:
                        conn.execute(
                            "UPDATE tasks SET status = ?, updated_at = ?, error_message = ? WHERE task_id = ?",
                            (status, now, error_message, task_id)
                        )
                logger.debug(f"ğŸ“ [TaskQueue] æ›´æ–°ä»»åŠ¡çŠ¶æ€: {task_id} -> {status}")
            except Exception as e:
                logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")

    def get_task(self, task_id: str) -> Optional[Dict]:
        """æ ¹æ®task_idè·å–ä»»åŠ¡è¯¦æƒ…"""
        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    conn.row_factory = sqlite3.Row
                    cursor = conn.cursor()
                    cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
                    row = cursor.fetchone()

                    if row:
                        task = {
                            "task_id": row["task_id"],
                            "task_type": row["task_type"],
                            "status": row["status"],
                            "payload": json.loads(row["payload"]) if row["payload"] else {},
                            "result": json.loads(row["result"]) if row["result"] else None,
                            "created_at": row["created_at"],
                            "updated_at": row["updated_at"],
                            "retry_count": row["retry_count"],
                            "error_message": row["error_message"]
                        }
                        return task
            except Exception as e:
                logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def get_pending_tasks(self) -> List[Dict]:
        """è·å–æœªå®Œæˆçš„ä»»åŠ¡ (ç”¨äºå¯åŠ¨æ—¶æ¢å¤)"""
        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    conn.row_factory = sqlite3.Row
                    cursor = conn.cursor()
                    cursor.execute("SELECT * FROM tasks WHERE status IN ('PENDING', 'RUNNING') ORDER BY created_at ASC")
                    rows = cursor.fetchall()

                    tasks = []
                    for row in rows:
                        try:
                            payload = json.loads(row["payload"]) if row["payload"] else {}
                        except:
                            payload = {}

                        tasks.append({
                            "task_id": row["task_id"],
                            "task_type": row["task_type"],
                            "payload": payload,
                            "created_at": row["created_at"],
                            "status": row["status"]
                        })
                    return tasks
            except Exception as e:
                logger.error(f"âŒ è·å–å¾…å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                return []

    def get_all_tasks(self, limit: int = 100, offset: int = 0) -> List[Dict]:
        """è·å–æ‰€æœ‰ä»»åŠ¡ï¼ˆåˆ†é¡µæŸ¥è¯¢ï¼‰"""
        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    conn.row_factory = sqlite3.Row
                    cursor = conn.cursor()
                    cursor.execute(
                        "SELECT * FROM tasks ORDER BY created_at DESC LIMIT ? OFFSET ?",
                        (limit, offset)
                    )
                    rows = cursor.fetchall()

                    tasks = []
                    for row in rows:
                        try:
                            payload = json.loads(row["payload"]) if row["payload"] else {}
                        except:
                            payload = {}

                        try:
                            result = json.loads(row["result"]) if row["result"] else None
                        except:
                            result = None

                        tasks.append({
                            "task_id": row["task_id"],
                            "task_type": row["task_type"],
                            "status": row["status"],
                            "payload": payload,
                            "result": result,
                            "created_at": row["created_at"],
                            "updated_at": row["updated_at"],
                            "retry_count": row["retry_count"]
                        })
                    return tasks
            except Exception as e:
                logger.error(f"âŒ è·å–æ‰€æœ‰ä»»åŠ¡å¤±è´¥: {e}")
                return []

    def clear_completed_tasks(self, days_to_keep: int = 7):
        """æ¸…ç†è¿‡æœŸçš„å·²å®Œæˆä»»åŠ¡"""
        cutoff = time.time() - (days_to_keep * 86400)
        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "DELETE FROM tasks WHERE status IN ('COMPLETED', 'FAILED') AND updated_at < ?",
                        (cutoff,)
                    )
                    deleted_count = cursor.rowcount
                    conn.commit()
                    logger.info(f"ğŸ§¹ [TaskQueue] æ¸…ç†äº† {deleted_count} ä¸ªè¿‡æœŸä»»åŠ¡")
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            try:
                with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
                    cursor = conn.cursor()

                    # ç»Ÿè®¡å„çŠ¶æ€ä»»åŠ¡æ•°é‡
                    cursor.execute("SELECT status, COUNT(*) as count FROM tasks GROUP BY status")
                    status_counts = {row[0]: row[1] for row in cursor.fetchall()}

                    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
                    cursor.execute("SELECT task_type, COUNT(*) as count FROM tasks GROUP BY task_type")
                    type_counts = {row[0]: row[1] for row in cursor.fetchall()}

                    # è·å–æœ€æ—§å’Œæœ€æ–°ä»»åŠ¡æ—¶é—´
                    cursor.execute("SELECT MIN(created_at), MAX(created_at) FROM tasks")
                    min_max = cursor.fetchone()

                    return {
                        "total_tasks": sum(status_counts.values()),
                        "status_counts": status_counts,
                        "type_counts": type_counts,
                        "oldest_task": min_max[0] if min_max and min_max[0] else None,
                        "newest_task": min_max[1] if min_max and min_max[1] else None,
                        "db_path": self.db_path
                    }
            except Exception as e:
                logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
                return {}


# ==========================================
# 2. çŸ¥è¯†ç¼“å­˜ç®¡ç†å™¨
# ==========================================

class KnowledgeKeeper:
    """çŸ¥è¯†ç¼“å­˜ç®¡ç†å™¨ï¼Œå®ç°æ–‡ä»¶å¢é‡æ›´æ–°å’Œç§’çº§å¯åŠ¨"""

    def __init__(self, io_manager):
        self.io_manager = io_manager
        self.cache_data = {}  # å†…å­˜ä¸­çš„ç¼“å­˜
        self.current_persona = "Default"
        self.cache_file_path = ""
        self._lock = threading.RLock()

    def load_persona_cache(self, persona_name: str):
        """åŠ è½½ç‰¹å®šäººæ ¼çš„çŸ¥è¯†ç¼“å­˜"""
        self.current_persona = persona_name

        # ç¼“å­˜æ–‡ä»¶å­˜æ”¾åœ¨: Inputs/{Persona}/.knowledge_index.json
        # å‰é¢åŠ ä¸ªç‚¹ï¼Œä½œä¸ºéšè—æ–‡ä»¶ï¼Œä¸è¢«å½“åšæ™®é€šæ–‡æ¡£è¯»å–
        if hasattr(self.io_manager, 'get_persona_folder'):
            persona_dir = self.io_manager.get_persona_folder(persona_name)
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬
            persona_dir = os.path.join(os.getcwd(), 'ATHENA_WORKSPACE', 'Inputs', persona_name)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(persona_dir):
            logger.warning(f"âš ï¸ [Cache] äººæ ¼ç›®å½•ä¸å­˜åœ¨: {persona_dir}")
            return

        self.cache_file_path = os.path.join(persona_dir, ".knowledge_index.json")

        if os.path.exists(self.cache_file_path):
            try:
                with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                    with self._lock:
                        self.cache_data = json.load(f)
                logger.info(f"âœ… [Cache] å·²åŠ è½½ {persona_name} çš„çŸ¥è¯†ç´¢å¼•ï¼ŒåŒ…å« {len(self.cache_data)} æ¡è®°å½•")
            except json.JSONDecodeError as e:
                logger.error(f"âŒ [Cache] ç´¢å¼•æ–‡ä»¶JSONæ ¼å¼é”™è¯¯ï¼Œå°†é‡å»º: {e}")
                self.cache_data = {}
                # å¤‡ä»½æŸåçš„æ–‡ä»¶
                backup_path = self.cache_file_path + f".backup.{int(time.time())}"
                os.rename(self.cache_file_path, backup_path)
                logger.info(f"ğŸ“ [Cache] å·²å¤‡ä»½æŸåæ–‡ä»¶è‡³: {backup_path}")
            except Exception as e:
                logger.error(f"âŒ [Cache] åŠ è½½ç´¢å¼•å¤±è´¥ï¼Œå°†é‡å»º: {e}")
                self.cache_data = {}
        else:
            logger.info(f"â„¹ï¸ [Cache] {persona_name} å°šæ— ç´¢å¼•ï¼Œå°†å»ºç«‹æ–°åº“")
            self.cache_data = {}

    def get_cached_record(self, file_path: str) -> Optional[Dict]:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æœ‰æ•ˆç¼“å­˜
        è¿”å›: ç¼“å­˜æ•°æ® dict æˆ– None
        """
        if not os.path.exists(file_path):
            return None

        filename = os.path.basename(file_path)
        current_mtime = os.path.getmtime(file_path)

        with self._lock:
            # æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
            if filename in self.cache_data:
                record = self.cache_data[filename]
                # ğŸ”¥ å…³é”®ï¼šæ¯”å¯¹ä¿®æ”¹æ—¶é—´ (ç²¾ç¡®åˆ°å°æ•°ç‚¹å4ä½)
                # å¦‚æœç¼“å­˜é‡Œçš„æ—¶é—´ == æ–‡ä»¶å®é™…æ—¶é—´ï¼Œè¯´æ˜æ²¡æ”¹è¿‡ï¼Œç›´æ¥ç”¨ç¼“å­˜
                cached_mtime = record.get("mtime", 0)
                if abs(cached_mtime - current_mtime) < 0.1:
                    return record

        return None  # éœ€è¦é‡æ–°æ‰«æ

    def check_cache(self, file_path: str) -> Optional[Dict]:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æœ‰æœ‰æ•ˆç¼“å­˜ï¼ˆå…¼å®¹æ€§åˆ«åï¼‰
        :return: ç¼“å­˜è®°å½• æˆ– None
        """
        return self.get_cached_record(file_path)

    def update_record(self, file_path: str, result_data: Dict[str, Any]):
        """
        åˆ†æå®Œæˆåï¼Œæ›´æ–°ç¼“å­˜è®°å½•
        """
        filename = os.path.basename(file_path)
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ [Cache] æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°ç¼“å­˜: {file_path}")
            return

        # æå–å…³é”®ä¿¡æ¯ç”¨äºUIå¿«é€Ÿæ¢å¤
        record = {
            "mtime": os.path.getmtime(file_path),
            "analyzed_at": time.time(),
            "file_path": file_path,  # ä¿å­˜å®Œæ•´è·¯å¾„ä»¥ä¾¿è¿½è¸ª
            # ä¿å­˜ç”¨äº UI æ˜¾ç¤ºçš„æ•°æ®
            "radar_metrics": result_data.get("radar_metrics", {}),
            "keywords": result_data.get("semantic_summary", {}).get("keywords", {}),
            "summary_text": result_data.get("text_report", "")[:500],  # å­˜ä¸ªæ‘˜è¦é¢„è§ˆ
            "dna_features": result_data.get("dna_features", {}),  # æ–°å¢DNAç‰¹å¾å­˜å‚¨
            # å¦‚æœéœ€è¦ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥å­˜ full_textï¼Œä½†ä¼šå¯¼è‡´ json å¾ˆå¤§
            # å»ºè®®ä¸å­˜å…¨æ–‡ï¼Œå…¨æ–‡æ£€ç´¢äº¤ç»™ RAG/å‘é‡åº“
        }

        with self._lock:
            self.cache_data[filename] = record
            self._save_to_disk()

        logger.debug(f"ğŸ“ [Cache] æ›´æ–°ç¼“å­˜è®°å½•: {filename}")

    def delete_record(self, file_path: str):
        """åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„ç¼“å­˜è®°å½•"""
        filename = os.path.basename(file_path)
        with self._lock:
            if filename in self.cache_data:
                del self.cache_data[filename]
                self._save_to_disk()
                logger.info(f"ğŸ—‘ï¸ [Cache] åˆ é™¤ç¼“å­˜è®°å½•: {filename}")

    def clear_cache(self):
        """æ¸…ç©ºå½“å‰äººæ ¼çš„ç¼“å­˜"""
        with self._lock:
            cache_size = len(self.cache_data)
            self.cache_data = {}
            self._save_to_disk()
            logger.info(f"ğŸ§¹ [Cache] æ¸…ç©ºäº† {cache_size} æ¡ç¼“å­˜è®°å½•")

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                "total_records": len(self.cache_data),
                "persona": self.current_persona,
                "cache_file": self.cache_file_path,
                "last_modified": os.path.getmtime(self.cache_file_path) if os.path.exists(
                    self.cache_file_path) else None
            }

    def _save_to_disk(self):
        """å†™å…¥ç£ç›˜"""
        if not self.cache_file_path:
            return

        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.cache_file_path), exist_ok=True)

            # å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åé‡å‘½åï¼Œç¡®ä¿åŸå­æ€§
            temp_path = self.cache_file_path + ".tmp"
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)

            # æ›¿æ¢åŸæ–‡ä»¶
            if os.path.exists(self.cache_file_path):
                os.remove(self.cache_file_path)
            os.rename(temp_path, self.cache_file_path)

        except Exception as e:
            logger.error(f"âŒ [Cache] ä¿å­˜å¤±è´¥: {e}")


# ==========================================
# 3. å…¨å±€ç®¡ç†å™¨å·¥å‚
# ==========================================

class PersistenceManager:
    """æŒä¹…åŒ–ç®¡ç†å™¨å·¥å‚ç±»ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†"""

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼"""
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(PersistenceManager, cls).__new__(cls)
            return cls._instance

    def __init__(self, io_manager=None):
        if not hasattr(self, '_initialized'):
            self.io_manager = io_manager
            self.knowledge_keeper = None
            self.task_queue = None
            self._initialized = True

            if io_manager:
                self.knowledge_keeper = KnowledgeKeeper(io_manager)

    def init_task_queue(self, db_path: str = None) -> TaskQueue:
        """åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæŒ‰éœ€åˆå§‹åŒ–ï¼‰"""
        if not self.task_queue:
            self.task_queue = TaskQueue(db_path)
        return self.task_queue

    def get_knowledge_keeper(self) -> KnowledgeKeeper:
        """è·å–çŸ¥è¯†ç¼“å­˜ç®¡ç†å™¨"""
        if not self.knowledge_keeper and self.io_manager:
            self.knowledge_keeper = KnowledgeKeeper(self.io_manager)
        return self.knowledge_keeper

    def get_task_queue(self) -> TaskQueue:
        """è·å–ä»»åŠ¡é˜Ÿåˆ—"""
        if not self.task_queue:
            self.task_queue = TaskQueue()
        return self.task_queue


# ==========================================
# ğŸ”¥ æ ¸å¿ƒä¿®æ­£ï¼šå®ä¾‹åŒ–å¹¶å¯¼å‡ºå…¨å±€å˜é‡
# ==========================================

# å…¨å±€å•ä¾‹å®ä¾‹
GLOBAL_TASK_QUEUE = TaskQueue()
GLOBAL_PERSISTENCE_MANAGER = PersistenceManager()


def get_task_queue() -> TaskQueue:
    """è·å–å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å®ä¾‹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    return GLOBAL_TASK_QUEUE


def get_persistence_manager(io_manager=None) -> PersistenceManager:
    """è·å–å…¨å±€æŒä¹…åŒ–ç®¡ç†å™¨å®ä¾‹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    if io_manager and not GLOBAL_PERSISTENCE_MANAGER.io_manager:
        GLOBAL_PERSISTENCE_MANAGER.io_manager = io_manager
    return GLOBAL_PERSISTENCE_MANAGER

