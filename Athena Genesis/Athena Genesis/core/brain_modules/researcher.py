# -*- coding: utf-8 -*-
"""
äº’è”å­¦ä¹ ä¸“å‘˜ - æœ¬åœ°çŸ¥è¯†æ£€ç´¢ + è”ç½‘æ·±åº¦å­¦ä¹ 
æ¨¡å—ç‰¹ç‚¹ï¼šä¸»åŠ¨å­¦ä¹ ã€çŸ¥è¯†ç¼ºå£åˆ†æã€æ™ºèƒ½æœç´¢
"""
import os
import re
import time
import json
import threading
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from PyQt6.QtCore import QObject, pyqtSignal
from datetime import datetime

# å°è¯•å¯¼å…¥ WebSearcher (å…¼å®¹æ€§å¤„ç†)
try:
    from core.web_searcher import WebSearcher
    WEB_SEARCHER_AVAILABLE = True
except ImportError:
    try:
        from engines.web_searcher import WebSearcher
        WEB_SEARCHER_AVAILABLE = True
    except ImportError:
        WebSearcher = None
        WEB_SEARCHER_AVAILABLE = False


class Researcher(QObject):
    """äº’è”å­¦ä¹ ä¸“å‘˜ - çŸ¥è¯†æ£€ç´¢ä¸ä¸»åŠ¨å­¦ä¹ """

    # ä¿¡å·å®šä¹‰
    log_signal = pyqtSignal(str)
    status_signal = pyqtSignal(str)
    search_complete = pyqtSignal(dict)

    def __init__(self, bus, llm, mimicry, analyzer, io_manager, knowledge_base):
        super().__init__()
        self.bus = bus
        self.llm = llm
        self.mimicry = mimicry
        self.analyzer = analyzer
        self.io_manager = io_manager
        self.kb = knowledge_base

        # è”ç½‘æœç´¢å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰ - ä½¿ç”¨ç»Ÿä¸€æ¥å£
        self.web_engine = None
        if WEB_SEARCHER_AVAILABLE:
            try:
                self.web_engine = WebSearcher(io_manager)
                self.log_signal.emit("âœ… [Researcher] è”ç½‘æœç´¢å¼•æ“å·²æŒ‚è½½")
            except Exception as e:
                self.log_signal.emit(f"âš ï¸ [Researcher] æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            self.log_signal.emit("âš ï¸ [Researcher] æœªæ‰¾åˆ° WebSearcher æ¨¡å—ï¼Œè”ç½‘åŠŸèƒ½å—é™")

        # å­¦ä¹ å†å²
        self.learning_history = []
        self.max_history = 50

        # å¹¶å‘é…ç½®
        self.executor = ThreadPoolExecutor(max_workers=3)

        # çŸ¥è¯†ç¼ºå£åº“
        self.gap_knowledge_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
            'data', 'knowledge_gaps.json'
        )

    def retrieve_knowledge(self, query, current_persona="é»˜è®¤ç©ºé—´", top_k=3):
        """
        å¢å¼ºç‰ˆçŸ¥è¯†æ£€ç´¢ - RAG + ä¸»åŠ¨å­¦ä¹ 

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            current_persona: å½“å‰äººæ ¼ç©ºé—´
            top_k: è¿”å›ç»“æœæ•°é‡
        Returns:
            RAGä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        self.status_signal.emit(f"ğŸ” æ£€ç´¢çŸ¥è¯†: {query[:30]}...")

        try:
            # 1. æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
            local_results = self._search_local(query, top_k)

            # 2. å¦‚æœæœ¬åœ°ç»“æœä¸è¶³ï¼Œä¸”å…è®¸è”ç½‘ï¼Œåˆ™å°è¯•è”ç½‘è¡¥å……
            if self._needs_web_supplement(local_results, query):
                self.log_signal.emit("ğŸŒ æœ¬åœ°çŸ¥è¯†ä¸è¶³ï¼Œå¯åŠ¨è”ç½‘è¡¥å……...")

                # å¼‚æ­¥è”ç½‘æœç´¢
                web_context = self._search_web_async(query)
                if web_context:
                    return f"ã€æœ¬åœ°çŸ¥è¯†ã€‘\n{local_results}\n\nã€è”ç½‘è¡¥å……ã€‘\n{web_context}"

            # 3. è¿”å›æœ¬åœ°ç»“æœ
            if local_results and "æœªæ‰¾åˆ°" not in local_results:
                return f"ã€å…³è”è®°å¿†æ£€ç´¢ ({current_persona})ã€‘\n{local_results}"

            return None

        except Exception as e:
            self.log_signal.emit(f"âŒ çŸ¥è¯†æ£€ç´¢å¤±è´¥: {str(e)}")
            return None

    def _search_local(self, query, top_k):
        """æœ¬åœ°çŸ¥è¯†åº“æœç´¢"""
        try:
            if hasattr(self.kb, 'search'):
                result = self.kb.search(query, top_k=top_k)
                return result if result and len(result) > 10 else "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
            else:
                # å¤‡ç”¨æœç´¢æ–¹æ¡ˆ
                return self._fallback_local_search(query)
        except Exception as e:
            return f"æœç´¢å¼‚å¸¸: {str(e)}"

    def _needs_web_supplement(self, local_results, query):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘è¡¥å……"""
        if not self.web_engine:
            return False

        # è§„åˆ™ï¼šå¦‚æœæœ¬åœ°ç»“æœå¤ªçŸ­æˆ–è€…åŒ…å«ç‰¹å®šå…³é”®è¯
        short_result = local_results and len(local_results) < 200
        has_web_keywords = any(word in query for word in ["æœ€æ–°", "2024", "è¶‹åŠ¿", "æ–°é—»", "è”ç½‘"])

        return short_result or has_web_keywords

    def _search_web_async(self, query):
        """å¼‚æ­¥è”ç½‘æœç´¢"""
        if not self.web_engine:
            return None

        try:
            # æå–æœç´¢å…³é”®è¯
            keywords = self._extract_search_keywords(query)

            # æ‰§è¡Œæœç´¢
            results = []
            for kw in keywords[:2]:  # æœ€å¤šæœç´¢2ä¸ªå…³é”®è¯
                self.log_signal.emit(f"ğŸŒ æ­£åœ¨æœç´¢: {kw}")

                try:
                    # ä½¿ç”¨WebSearcherçš„searchæ–¹æ³•
                    if hasattr(self.web_engine, 'search'):
                        search_result = self.web_engine.search(kw, max_results=3)
                        # æ ¼å¼åŒ–ç»“æœ
                        formatted_result = self._format_web_results(search_result)
                        if formatted_result:
                            results.append(f"ã€{kw}ã€‘:\n{formatted_result[:500]}")
                    time.sleep(0.5)  # ç¤¼è²Œå»¶æ—¶
                except Exception as e:
                    self.log_signal.emit(f"âš ï¸ æœç´¢ '{kw}' å¤±è´¥: {e}")

            return "\n\n".join(results) if results else None

        except Exception as e:
            self.log_signal.emit(f"âŒ è”ç½‘æœç´¢å¼‚å¸¸: {e}")
            return None

    def _format_web_results(self, results):
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        if isinstance(results, str):
            return results
        if not isinstance(results, list):
            return ""

        text = ""
        for i, res in enumerate(results):
            if isinstance(res, dict):
                title = res.get('title', 'æ— æ ‡é¢˜')
                body = res.get('body', res.get('snippet', res.get('content', '')))
                link = res.get('href', res.get('link', ''))
                text += f"[{i+1}] {title}\næ‘˜è¦: {body}\næ¥æº: {link}\n\n"
            elif isinstance(res, str):
                text += f"[{i+1}] {res}\n"
        return text

    def _extract_search_keywords(self, query):
        """ä»æŸ¥è¯¢ä¸­æå–æœç´¢å…³é”®è¯"""
        prompt = f"""
        è¯·ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–2-3ä¸ªæ ¸å¿ƒæœç´¢å…³é”®è¯ï¼š

        é—®é¢˜ï¼š{query}

        è¦æ±‚ï¼š
        1. å»æ‰ç–‘é—®è¯ï¼ˆä»€ä¹ˆã€å¦‚ä½•ã€ä¸ºä»€ä¹ˆç­‰ï¼‰
        2. æå–æ ¸å¿ƒåè¯å’ŒåŠ¨è¯
        3. æŒ‰é‡è¦æ€§æ’åº
        4. æ¯ä¸ªå…³é”®è¯ä¸è¶…è¿‡4ä¸ªå­—

        æ ¼å¼ï¼šå…³é”®è¯1, å…³é”®è¯2, å…³é”®è¯3
        """

        try:
            response = self.llm.chat(prompt, options={"temperature": 0.1})
            keywords = [kw.strip() for kw in response.split(',') if kw.strip()]
            return keywords[:3] if keywords else [query[:10]]
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•åˆ†è¯
            try:
                import jieba
                words = jieba.lcut_for_search(query)
                return [w for w in words if len(w) > 1][:3]
            except:
                # æœ€åå¤‡é€‰
                return [query[:10]]

    def simple_answer(self, query, use_web=True):
        """
        ç®€å•è”ç½‘é—®ç­”æ¨¡å¼
        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_web: æ˜¯å¦ä½¿ç”¨è”ç½‘æœç´¢
        Returns:
            å›ç­”æ–‡æœ¬
        """
        self.log_signal.emit(f"ğŸŒ æ‰§è¡Œç®€å•é—®ç­”: {query[:50]}...")

        try:
            # 1. å°è¯•æœ¬åœ°çŸ¥è¯†åº“
            local_answer = self._search_local(query, top_k=2)

            # 2. å¦‚æœéœ€è¦ä¸”å…è®¸ï¼Œè¿›è¡Œè”ç½‘æœç´¢
            if use_web and self.web_engine:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æœ€æ–°ä¿¡æ¯
                if self._needs_fresh_info(query):
                    web_context = self._search_web_async(query)

                    if web_context:
                        # åˆå¹¶æœ¬åœ°å’Œç½‘ç»œä¿¡æ¯
                        combined_context = f"{local_answer}\n\nç½‘ç»œè¡¥å……:\n{web_context}"
                        answer = self._synthesize_answer(query, combined_context)
                        return answer

            # 3. å¦‚æœä¸éœ€è¦è”ç½‘æˆ–è”ç½‘å¤±è´¥ï¼Œç›´æ¥å›ç­”
            if local_answer and "æœªæ‰¾åˆ°" not in local_answer:
                answer = self._synthesize_answer(query, local_answer)
                return answer

            # 4. å®åœ¨æ‰¾ä¸åˆ°ï¼Œè¿”å›é€šç”¨å›å¤
            return "æŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·å°è¯•æ›´å…·ä½“çš„é—®é¢˜ï¼Œæˆ–è€…å¯ç”¨è”ç½‘æœç´¢åŠŸèƒ½ã€‚"

        except Exception as e:
            self.log_signal.emit(f"âŒ ç®€å•é—®ç­”å¤±è´¥: {str(e)}")
            return f"å›ç­”è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}"

    def _needs_fresh_info(self, query):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æœ€æ–°ä¿¡æ¯"""
        fresh_keywords = [
            "æœ€æ–°", "ä»Šå¤©", "è¿‘æœŸ", "2024", "2025", "ä»Šå¹´",
            "æ–°é—»", "åŠ¨æ€", "è¶‹åŠ¿", "æ›´æ–°", "åˆšåˆš", "æœ€è¿‘"
        ]

        query_lower = query.lower()
        return any(keyword in query_lower for keyword in fresh_keywords)

    def _synthesize_answer(self, query, context):
        """åŸºäºä¸Šä¸‹æ–‡åˆæˆå›ç­”"""
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”é—®é¢˜ï¼š

        ã€é—®é¢˜ã€‘ï¼š
        {query}

        ã€å‚è€ƒä¿¡æ¯ã€‘ï¼š
        {context}

        ã€è¦æ±‚ã€‘ï¼š
        1. å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜
        2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
        3. ä¿æŒå®¢è§‚ä¸­ç«‹
        4. é•¿åº¦æ§åˆ¶åœ¨300å­—ä»¥å†…
        """

        try:
            answer = self.llm.chat(prompt, options={"temperature": 0.3})
            return answer.strip()
        except Exception as e:
            self.log_signal.emit(f"âŒ å›ç­”åˆæˆå¤±è´¥: {e}")
            return context[:200] + "..."  # è¿”å›åŸå§‹ä¿¡æ¯ç‰‡æ®µ

    def deep_learn(self, topic, local_context="", depth="medium"):
        """
        æ·±åº¦å­¦ä¹ æ¨¡å¼ - ä¸»åŠ¨å¡«è¡¥çŸ¥è¯†ç¼ºå£

        Args:
            topic: å­¦ä¹ ä¸»é¢˜
            local_context: å·²æœ‰æœ¬åœ°ä¸Šä¸‹æ–‡
            depth: "quick"å¿«é€Ÿå­¦ä¹  / "medium"ä¸­ç­‰æ·±åº¦ / "deep"æ·±åº¦å­¦ä¹ 
        Returns:
            å­¦ä¹ åˆ°çš„çŸ¥è¯†
        """
        self.log_signal.emit(f"ğŸ§  å¯åŠ¨æ·±åº¦å­¦ä¹ : {topic}")

        try:
            # 1. åˆ†æçŸ¥è¯†ç¼ºå£
            gap_analysis = self._analyze_knowledge_gap(topic, local_context)

            # 2. æ ¹æ®æ·±åº¦å†³å®šæœç´¢ç­–ç•¥
            if depth == "quick":
                search_keywords = gap_analysis.get("keywords", [topic])[:2]
            elif depth == "medium":
                search_keywords = gap_analysis.get("keywords", [topic])[:4]
            else:  # deep
                search_keywords = gap_analysis.get("keywords", [topic])[:6]

            # 3. å¹¶è¡Œæœç´¢å­¦ä¹ 
            learned_materials = self._parallel_learn(search_keywords, depth)

            # 4. çŸ¥è¯†æ•´åˆä¸æç‚¼
            integrated_knowledge = self._integrate_knowledge(
                topic, local_context, learned_materials
            )

            # 5. è®°å½•å­¦ä¹ å†å²
            self._record_learning(topic, search_keywords, len(learned_materials))

            return integrated_knowledge

        except Exception as e:
            self.log_signal.emit(f"âŒ æ·±åº¦å­¦ä¹ å¤±è´¥: {str(e)}")
            return f"å­¦ä¹ è¿‡ç¨‹å‡ºé”™: {str(e)}"

    def _analyze_knowledge_gap(self, topic, local_context):
        """åˆ†æçŸ¥è¯†ç¼ºå£"""
        prompt = f"""
        ä¸»é¢˜ï¼š{topic}
        å·²æœ‰èµ„æ–™ï¼š{local_context[:500] if local_context else "æ— "}

        è¯·åˆ†æè¿˜éœ€è¦å“ªäº›æ–¹é¢çš„çŸ¥è¯†ï¼Œè¾“å‡ºæ ¼å¼ï¼š

        ã€çŸ¥è¯†ç¼ºå£åˆ†æã€‘ï¼š
        1. æ ¸å¿ƒæ¦‚å¿µæ¾„æ¸…ï¼šéœ€è¦æ˜ç¡®çš„å®šä¹‰å’Œè¾¹ç•Œ
        2. æœ€æ–°è¿›å±•ï¼šéœ€è¦äº†è§£çš„æœ€æ–°å‘å±•
        3. å®è·µæ¡ˆä¾‹ï¼šéœ€è¦å…·ä½“çš„åº”ç”¨æ¡ˆä¾‹
        4. ç›¸å…³ç†è®ºï¼šéœ€è¦äº†è§£çš„æ”¯æ’‘ç†è®º

        ã€æœç´¢å…³é”®è¯å»ºè®®ã€‘ï¼šå…³é”®è¯1, å…³é”®è¯2, å…³é”®è¯3...
        """

        try:
            response = self.llm.chat(prompt, options={"temperature": 0.2})

            # è§£æå“åº”
            gaps = {}
            keywords = [topic]  # é»˜è®¤åŒ…å«ä¸»é¢˜

            if "ã€æœç´¢å…³é”®è¯å»ºè®®ã€‘" in response:
                kw_part = response.split("ã€æœç´¢å…³é”®è¯å»ºè®®ã€‘")[1].strip()
                extracted = re.findall(r'[^,ï¼Œ\s]+', kw_part)
                keywords.extend([k.strip() for k in extracted if len(k.strip()) > 1])

            return {
                "analysis": response[:500],
                "keywords": list(set(keywords))[:8]  # å»é‡å¹¶é™åˆ¶æ•°é‡
            }

        except Exception as e:
            self.log_signal.emit(f"âš ï¸ ç¼ºå£åˆ†æå¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯: {e}")
            return {"keywords": [topic, "æœ€æ–°å‘å±•", "å®è·µæ¡ˆä¾‹"]}

    def _parallel_learn(self, keywords, depth):
        """å¹¶è¡Œå­¦ä¹ å¤šä¸ªå…³é”®è¯"""
        learned_materials = []

        # ç¡®å®šæ¯ä¸ªå…³é”®è¯çš„æœç´¢å¼ºåº¦
        if depth == "quick":
            max_results = 1
            max_length = 300
        elif depth == "medium":
            max_results = 2
            max_length = 500
        else:  # deep
            max_results = 3
            max_length = 800

        # å¹¶è¡Œæœç´¢ä»»åŠ¡
        futures = {}
        for kw in keywords[:6]:  # æœ€å¤š6ä¸ªå…³é”®è¯
            future = self.executor.submit(
                self._single_keyword_learn,
                kw, max_results, max_length
            )
            futures[future] = kw

        # æ”¶é›†ç»“æœ
        for future in as_completed(futures):
            kw = futures[future]
            try:
                result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                if result:
                    learned_materials.append(f"ã€{kw}ã€‘:\n{result}")
            except Exception as e:
                self.log_signal.emit(f"âš ï¸ å­¦ä¹ å…³é”®è¯ '{kw}' å¤±è´¥: {e}")

        return learned_materials

    def _single_keyword_learn(self, keyword, max_results, max_length):
        """å•ä¸ªå…³é”®è¯å­¦ä¹ """
        if not self.web_engine:
            return f"æœªå¯ç”¨è”ç½‘æœç´¢ï¼Œæ— æ³•å­¦ä¹ : {keyword}"

        try:
            # ä½¿ç”¨WebSearcherçš„searchæ–¹æ³•
            if hasattr(self.web_engine, 'search'):
                raw_results = self.web_engine.search(keyword, max_results=max_results)
                if raw_results:
                    # æç‚¼å¹²è´§
                    summary = self._summarize_raw_data(raw_results, max_length)
                    return summary
        except Exception as e:
            self.log_signal.emit(f"âŒ å­¦ä¹ '{keyword}'å¼‚å¸¸: {e}")

        return None

    def _summarize_raw_data(self, raw_data, max_length):
        """æç‚¼ç½‘ç»œæ•°æ®"""
        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œç›´æ¥å¤„ç†
        if isinstance(raw_data, str):
            content = raw_data
        elif isinstance(raw_data, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå…ˆæ ¼å¼åŒ–
            content = self._format_web_results(raw_data)
        else:
            content = str(raw_data)

        prompt = f"""
        è¯·æç‚¼ä»¥ä¸‹å†…å®¹çš„å¹²è´§ï¼Œå»é™¤å¹¿å‘Šã€åºŸè¯å’Œé‡å¤ä¿¡æ¯ï¼š

        {content[:2000]}

        è¦æ±‚ï¼š
        1. æå–æ ¸å¿ƒäº‹å®å’Œè§‚ç‚¹
        2. ä¿æŒå®¢è§‚å‡†ç¡®
        3. é•¿åº¦ä¸è¶…è¿‡{max_length}å­—
        4. æŒ‰é‡è¦æ€§æ’åº
        """

        try:
            summary = self.llm.chat(prompt, options={"temperature": 0.1})
            return summary.strip()
        except:
            return content[:max_length]  # å¦‚æœæç‚¼å¤±è´¥ï¼Œè¿”å›åŸå§‹ç‰‡æ®µ

    def _integrate_knowledge(self, topic, local_context, learned_materials):
        """æ•´åˆå­¦ä¹ åˆ°çš„çŸ¥è¯†"""
        all_materials = "\n\n".join(learned_materials)

        prompt = f"""
        ä¸»é¢˜ï¼š{topic}

        ã€å·²æœ‰æœ¬åœ°çŸ¥è¯†ã€‘ï¼š
        {local_context if local_context else "æš‚æ— "}

        ã€æ–°å­¦ä¹ åˆ°çš„çŸ¥è¯†ã€‘ï¼š
        {all_materials}

        ã€ä»»åŠ¡ã€‘ï¼šæ•´åˆä»¥ä¸Šæ‰€æœ‰çŸ¥è¯†ï¼Œå½¢æˆä¸€ä¸ªç»“æ„åŒ–çš„çŸ¥è¯†æ‘˜è¦ã€‚

        ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
        ä¸€ã€æ ¸å¿ƒæ¦‚å¿µ
        äºŒã€å…³é”®äº‹å®
        ä¸‰ã€å®è·µå¯ç¤º
        å››ã€å­¦ä¹ å»ºè®®

        ã€è¦æ±‚ã€‘ï¼š
        1. å‡†ç¡®å¼•ç”¨æ¥æº
        2. å»é™¤çŸ›ç›¾ä¿¡æ¯
        3. çªå‡ºé‡ç‚¹
        4. ç»“æ„æ¸…æ™°
        """

        try:
            integrated = self.llm.chat(prompt, options={"temperature": 0.3})
            return integrated
        except Exception as e:
            self.log_signal.emit(f"âŒ çŸ¥è¯†æ•´åˆå¤±è´¥: {e}")
            return all_materials  # è¿”å›åŸå§‹å­¦ä¹ ææ–™

    def _record_learning(self, topic, keywords, material_count):
        """è®°å½•å­¦ä¹ å†å²"""
        record = {
            "topic": topic,
            "keywords": keywords,
            "material_count": material_count,
            "timestamp": datetime.now().isoformat(),
            "type": "deep_learn"
        }

        self.learning_history.append(record)

        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.learning_history) > self.max_history:
            self.learning_history = self.learning_history[-self.max_history:]

    def _fallback_local_search(self, query):
        """å¤‡ç”¨æœ¬åœ°æœç´¢æ–¹æ¡ˆ"""
        try:
            # å¦‚æœçŸ¥è¯†åº“æœ‰dataå±æ€§ï¼Œå°è¯•ç›´æ¥æœç´¢
            if hasattr(self.kb, 'data') and 'documents' in self.kb.data:
                docs = self.kb.data['documents']

                # ç®€å•å…³é”®è¯åŒ¹é…
                results = []
                query_words = set(query.lower().split())

                for doc_name, doc_data in docs.items():
                    content = doc_data.get('content', '').lower()

                    # è®¡ç®—åŒ¹é…åº¦
                    match_count = sum(1 for word in query_words if word in content)
                    if match_count > 0:
                        snippet = doc_data.get('summary', content[:100])
                        results.append(f"ã€Š{doc_name}ã€‹: {snippet}")

                if results:
                    return "\n".join(results[:3])  # è¿”å›å‰3ä¸ªç»“æœ

            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        except Exception as e:
            return f"æœç´¢å¼‚å¸¸: {str(e)}"

    def get_learning_stats(self):
        """è·å–å­¦ä¹ ç»Ÿè®¡"""
        total_searches = len(self.learning_history)
        recent_topics = [h['topic'] for h in self.learning_history[-5:]]

        return {
            "total_searches": total_searches,
            "recent_topics": recent_topics,
            "active": total_searches > 0
        }

    def clear_history(self):
        """æ¸…ç©ºå­¦ä¹ å†å²"""
        self.learning_history = []
        self.log_signal.emit("ğŸ§¹ å·²æ¸…ç©ºå­¦ä¹ å†å²")

    def research(self, query: str, deep=False) -> str:
        """
        æ‰§è¡Œç ”ç©¶ä»»åŠ¡ - æ–°å¢çš„æ·±åº¦æ€è€ƒæ¨¡å¼
        :param query: ç ”ç©¶è¯¾é¢˜
        :param deep: æ˜¯å¦æ·±åº¦æ¨¡å¼ (å¤šæºæ•´åˆ)
        :return: ç ”ç©¶æŠ¥å‘Š
        """
        self.log_signal.emit(f"ğŸ” [Researcher] å¼€å§‹ç ”ç©¶è¯¾é¢˜: {query}")

        # 1. å…ˆæŸ¥æœ¬åœ°çŸ¥è¯†åº“
        local_context = ""
        if hasattr(self.kb, 'search'):
            local_context = self.kb.search(query, limit=3)
            if local_context:
                self.log_signal.emit("ğŸ“š å·²æå–æœ¬åœ°ç›¸å…³çŸ¥è¯†")

        # 2. è”ç½‘æœç´¢ (å¦‚æœå¯ç”¨)
        web_context = ""
        if self.web_engine and deep:
            self.status_signal.emit("æ­£åœ¨è”ç½‘æœç´¢...")
            try:
                if hasattr(self.web_engine, 'search'):
                    web_results = self.web_engine.search(query, max_results=5)
                    web_context = self._format_web_results(web_results)
                    self.log_signal.emit(f"ğŸŒ è”ç½‘è·å–äº† {len(web_results)} æ¡ä¿¡æ¯")
            except Exception as e:
                self.log_signal.emit(f"âš ï¸ è”ç½‘æœç´¢å¼‚å¸¸: {e}")

        # 3. ç»¼åˆç”Ÿæˆå›ç­”
        self.status_signal.emit("æ­£åœ¨æ•´åˆæŠ¥å‘Š...")
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ·±åº¦ç ”ç©¶å‘˜ã€‚è¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€ç”¨æˆ·è¯¾é¢˜ã€‘ï¼š{query}

ã€æœ¬åœ°çŸ¥è¯†åº“ä¿¡æ¯ã€‘ï¼š
{local_context}

ã€äº’è”ç½‘æœ€æ–°ä¿¡æ¯ã€‘ï¼š
{web_context}

è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“å’Œäº’è”ç½‘ä¿¡æ¯ã€‚
2. å¦‚æœä¿¡æ¯å†²çªï¼Œä»¥æœ€æ–°çš„äº’è”ç½‘ä¿¡æ¯ä¸ºå‡†ã€‚
3. ç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹è®ºè¿°ã€‚
4. æ³¨æ˜ä¿¡æ¯æ¥æºï¼ˆæœ¬åœ°/ç½‘ç»œï¼‰ã€‚
"""
        try:
            response = self.llm.chat(prompt)

            # è®°å½•å†å²
            self.learning_history.append({
                "topic": query,
                "timestamp": time.time(),
                "has_web": bool(web_context),
                "type": "research"
            })

            return response
        except Exception as e:
            self.log_signal.emit(f"âŒ ç ”ç©¶è¿‡ç¨‹å¤±è´¥: {e}")
            return f"ç ”ç©¶è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}"

    def deep_research(self, query: str, outline_mode=True) -> str:
        """
        æ·±åº¦ç ”ç©¶æ¨¡å¼ - é…åˆPhase 2çš„å¤§çº²æ¨¡å¼
        :param query: ç ”ç©¶è¯¾é¢˜
        :param outline_mode: æ˜¯å¦ä½¿ç”¨å¤§çº²æ¨¡å¼
        :return: æ·±åº¦ç ”ç©¶æŠ¥å‘Š
        """
        self.log_signal.emit(f"ğŸ”¬ [Researcher] å¯åŠ¨æ·±åº¦ç ”ç©¶: {query}")

        # ç”Ÿæˆç ”ç©¶å¤§çº²
        if outline_mode:
            outline = self._generate_research_outline(query)
            self.log_signal.emit(f"ğŸ“‹ ç”Ÿæˆç ”ç©¶å¤§çº²: {outline[:100]}...")

            # å¯¹æ¯ä¸ªå¤§çº²é¡¹è¿›è¡Œç ”ç©¶
            sections = []
            for i, section in enumerate(self._extract_outline_sections(outline)):
                self.status_signal.emit(f"ç ”ç©¶ç« èŠ‚ {i+1}: {section[:30]}...")
                section_content = self.research(f"{query} - {section}", deep=True)
                sections.append(f"## {section}\n{section_content}")

            # æ•´åˆæŠ¥å‘Š
            report = f"# {query}\n\n## ç ”ç©¶å¤§çº²\n{outline}\n\n" + "\n\n".join(sections)
        else:
            # ç›´æ¥æ·±åº¦ç ”ç©¶
            report = self.research(query, deep=True)

        # è®°å½•æ·±åº¦ç ”ç©¶å†å²
        self.learning_history.append({
            "topic": query,
            "timestamp": time.time(),
            "type": "deep_research",
            "outline_mode": outline_mode
        })

        return report

    def _generate_research_outline(self, query):
        """ç”Ÿæˆç ”ç©¶å¤§çº²"""
        prompt = f"""
        è¯·ä¸ºä»¥ä¸‹ç ”ç©¶è¯¾é¢˜ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ç ”ç©¶å¤§çº²ï¼š

        ç ”ç©¶è¯¾é¢˜ï¼š{query}

        è¦æ±‚ï¼š
        1. å¤§çº²åº”åŒ…å«3-5ä¸ªä¸»è¦ç« èŠ‚
        2. æ¯ä¸ªç« èŠ‚åº”æœ‰2-3ä¸ªå­ç« èŠ‚
        3. å¤§çº²åº”é€»è¾‘æ¸…æ™°ï¼Œè¦†ç›–è¯¾é¢˜çš„å„ä¸ªæ–¹é¢
        4. æ ¼å¼ä½¿ç”¨Markdownæ ‡é¢˜æ ¼å¼

        ä¾‹å¦‚ï¼š
        # ç ”ç©¶è¯¾é¢˜
        ## ç¬¬ä¸€ç« ï¼šå¼•è¨€
        ### 1.1 ç ”ç©¶èƒŒæ™¯
        ### 1.2 ç ”ç©¶æ„ä¹‰
        ## ç¬¬äºŒç« ï¼šç†è®ºåŸºç¡€
        ### 2.1 æ ¸å¿ƒæ¦‚å¿µ
        ### 2.2 ç›¸å…³ç†è®º
        ## ç¬¬ä¸‰ç« ï¼šç°çŠ¶åˆ†æ
        ### 3.1 å½“å‰å‘å±•çŠ¶å†µ
        ### 3.2 å­˜åœ¨é—®é¢˜
        ## ç¬¬å››ç« ï¼šè§£å†³æ–¹æ¡ˆ
        ### 4.1 å»ºè®®æªæ–½
        ### 4.2 å®æ–½æ­¥éª¤
        ## ç¬¬äº”ç« ï¼šç»“è®º
        ### 5.1 ç ”ç©¶æ€»ç»“
        ### 5.2 æœªæ¥å±•æœ›
        """

        try:
            outline = self.llm.chat(prompt, options={"temperature": 0.3})
            return outline
        except Exception as e:
            self.log_signal.emit(f"âš ï¸ å¤§çº²ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å¤§çº²
            return f"""
            # {query}
            ## ç¬¬ä¸€ç« ï¼šå¼•è¨€
            ## ç¬¬äºŒç« ï¼šæ ¸å¿ƒæ¦‚å¿µ
            ## ç¬¬ä¸‰ç« ï¼šç°çŠ¶åˆ†æ
            ## ç¬¬å››ç« ï¼šè§£å†³æ–¹æ¡ˆ
            ## ç¬¬äº”ç« ï¼šç»“è®º
            """

    def _extract_outline_sections(self, outline):
        """ä»å¤§çº²ä¸­æå–ç« èŠ‚æ ‡é¢˜"""
        sections = []
        lines = outline.split('\n')
        for line in lines:
            # åŒ¹é…äºŒçº§å’Œä¸‰çº§æ ‡é¢˜
            if line.strip().startswith('## ') and not line.strip().startswith('###'):
                section = line.strip()[3:].strip()
                if section and len(section) > 2:
                    sections.append(section)
        return sections[:5]  # æœ€å¤š5ä¸ªç« èŠ‚