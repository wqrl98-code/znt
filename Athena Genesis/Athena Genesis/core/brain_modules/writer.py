# -*- coding: utf-8 -*-
"""
é¦–å¸­æ‰§ç¬”äºº - è´Ÿè´£æ ¹æ®çŸ¥è¯†åº“å’Œäººç‰©ç”Ÿæˆåˆç¨¿
èŒè´£ï¼šé•¿æ–‡ç”Ÿæˆã€ç»­å†™ã€å…¨åŸŸæ·±åº¦ç†”ç‚‰ã€é‡‘å­—å¡”å†™ä½œã€æ·±åº¦ç ”æŠ¥æ¨¡å¼ã€æ™ºèƒ½å†™ä½œ
"""
import os
import re
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from config.genres import get_genre_config
from PyQt6.QtCore import QObject, pyqtSignal


class Writer(QObject):
    """é¦–å¸­æ‰§ç¬”äºº - æ–‡æœ¬ç”Ÿæˆä¸“å®¶"""

    # PyQt6 ä¿¡å·
    log_signal = pyqtSignal(str)
    progress_signal = pyqtSignal(str)

    def __init__(self, bus, llm, mimicry, io_manager, knowledge_base):
        super().__init__()
        self.bus = bus
        self.llm = llm
        self.mimicry = mimicry
        self.io_manager = io_manager
        self.knowledge_base = knowledge_base

        # ç»­å†™ç¼“å­˜
        self.last_generation_tail = ""
        self.last_system_role = ""

        # å…¨åŸŸç†”ç‚‰ä¸Šä¸‹æ–‡
        self.last_global_furnace_context = None

        # å¹¶å‘é…ç½®
        self.MAX_IO_WORKERS = 8
        self.MAX_LLM_WORKERS = 5

    def _emit_log(self, message):
        """ç»Ÿä¸€æ—¥å¿—å‘å°„æ–¹æ³•"""
        self.bus.emit_log(message)
        self.log_signal.emit(message)

    def handle_continuation(self, tail, system_role):
        """
        å¤„ç†ç»­å†™è¯·æ±‚
        """
        self._emit_log("âš¡ [æ‰§ç¬”äºº] è¯†åˆ«åˆ°ç»­å†™æŒ‡ä»¤ï¼Œå¯åŠ¨è½»é‡çº§ç»­å†™æ¨¡å¼...")

        prompt = f"""
        ä½ ç°åœ¨çš„èº«ä»½æ˜¯ï¼š{system_role}

        ã€ä¸Šæ–‡èƒŒæ™¯ã€‘ï¼š
        ...ï¼ˆå‰æ–‡çœç•¥ï¼‰...
        {tail}

        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š
        è¯·ç´§æ¥ä¸Šæ–‡ï¼Œç»§ç»­æ’°å†™æœªå®Œæˆçš„å†…å®¹ã€‚

        ã€è¦æ±‚ã€‘ï¼š
        1. é€»è¾‘ä¸¥ä¸åˆç¼ï¼Œä¸è¦é‡å¤ä¸Šæ–‡æœ€åä¸€å¥ã€‚
        2. å¦‚æœä¸Šæ–‡æ˜¯åœ¨åŠå¥è¯æ–­å¼€çš„ï¼Œè¯·è¡¥å…¨å®ƒã€‚
        3. ä¿æŒåŸæœ‰æ–‡é£ã€‚
        4. è¾“å‡ºå­—æ•°çº¦800-1000å­—ã€‚
        """

        try:
            continuation = self.llm.chat(prompt, options={"temperature": 0.6})

            # æ›´æ–°å°¾å·´
            self.last_generation_tail = continuation[-1000:] if len(continuation) > 1000 else continuation

            # æ·»åŠ äº¤äº’å¼å¼•å¯¼
            interactive_footer = (
                "\n\n"
                "---"
                "\n> **âš ï¸ ç³»ç»Ÿç›‘æµ‹**ï¼šå¦‚æœä¸Šè¿°å†…å®¹æœªæ˜¾ç¤ºå®Œæ•´ï¼Œæˆ–è€…æ‚¨å¸Œæœ›ç»§ç»­æ‰©å†™ï¼šè¯·ç›´æ¥å›å¤"
                "\n>  ç»§ç»­,æ˜¯æˆ‘å°†æ— ç¼æ¥ç»­ç”Ÿæˆã€‚"
            )

            return str(continuation + interactive_footer)
        except Exception as e:
            return f"âŒ ç»­å†™å¤±è´¥: {str(e)}"

    def intelligent_write(self, topic: str, strategy: dict) -> str:
        """
        æ™ºèƒ½å†™ä½œä¸»å…¥å£
        :param topic: å†™ä½œä¸»é¢˜/ç”¨æˆ·è¾“å…¥
        :param strategy: ç­–ç•¥é…ç½® (audience, tone, genre, etc.)
        :return: æœ€ç»ˆæ–‡ç« å†…å®¹
        """
        self._emit_log(f"âœï¸ [æ‰§ç¬”äºº] æ”¶åˆ°ä»»åŠ¡ï¼š{topic[:20]}... | ç­–ç•¥ï¼š{strategy}")

        # 1. è·å–é£æ ¼ DNA (å¦‚æœæœ‰)
        style_instruction = ""
        if hasattr(self.mimicry, 'get_style_instruction'):
            style_instruction = self.mimicry.get_style_instruction()
            if style_instruction:
                self._emit_log("ğŸ§¬ [æ‹Ÿæ€] å·²æ³¨å…¥é£æ ¼ DNA")

        # 2. æ„å»ºç³»ç»Ÿæç¤ºè¯ (System Prompt)
        genre = strategy.get('genre', 'é€šç”¨')
        audience = strategy.get('audience', 'é€šç”¨è¯»è€…')
        tone = strategy.get('tone', 'å®¢è§‚')

        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†™ä½œä¸“å®¶ã€‚
ã€ç›®æ ‡å—ä¼—ã€‘ï¼š{audience}
ã€è¯­è°ƒé£æ ¼ã€‘ï¼š{tone}
ã€æ–‡ç« ç±»å‹ã€‘ï¼š{genre}

{style_instruction}

è¯·ä¸¥æ ¼éµå®ˆä¸Šè¿°é£æ ¼è¦æ±‚ã€‚
"""

        # 3. ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº² (Outline)
        self.progress_signal.emit("æ­£åœ¨æ„å»ºå¤§çº²...")
        outline_prompt = f"""
è¯·ä¸ºä¸»é¢˜"{topic}"è®¾è®¡ä¸€ä»½è¯¦ç»†çš„å†™ä½œå¤§çº²ã€‚
è¦æ±‚ï¼š
1. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘é€’è¿›ã€‚
2. åˆ—å‡ºæ ¸å¿ƒè§‚ç‚¹å’Œæ”¯æ’‘è®ºæ®ã€‚
3. ä¸éœ€è¦å†™æ­£æ–‡ï¼Œåªéœ€è¦å¤§çº²ã€‚
"""
        outline = self.llm.chat(outline_prompt, system_prompt=system_prompt)
        self._emit_log(f"ğŸ“ å¤§çº²å·²ç”Ÿæˆ ({len(outline)}å­—)")

        # 4. ç¬¬äºŒæ­¥ï¼šåŸºäºå¤§çº²ç”Ÿæˆå…¨æ–‡ (Full Text)
        self.progress_signal.emit("æ­£åœ¨æ ¹æ®å¤§çº²æ’°å†™æ­£æ–‡...")
        write_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹å¤§çº²ï¼Œæ’°å†™ä¸€ç¯‡å®Œæ•´çš„æ–‡ç« ã€‚
ã€ä¸»é¢˜ã€‘ï¼š{topic}

ã€å¤§çº²ã€‘ï¼š
{outline}

ã€è¦æ±‚ã€‘ï¼š
1. å†…å®¹å……å®ï¼Œç»†èŠ‚ä¸°å¯Œã€‚
2. ä¸¥æ ¼éµå¾ªå¤§çº²ç»“æ„ã€‚
3. ä¿æŒè®¾å®šçš„äººæ ¼å’Œè¯­è°ƒã€‚
"""
        # è¿™é‡Œçš„ timeout å¯ä»¥è®¾ç½®å¾—æ›´é•¿ï¼Œç”± LLMEngine å†…éƒ¨æ§åˆ¶
        article = self.llm.chat(write_prompt, system_prompt=system_prompt)

        self._emit_log("âœ… å†™ä½œä»»åŠ¡å®Œæˆ")
        return article

    def create_outline(self, query, context="", genre_config=None):
        """
        æ·±åº¦ç ”æŠ¥æ¨¡å¼ä¸“ç”¨ï¼šæ„å»ºå†…å®¹å¤§çº²

        Args:
            query: ä¸»é¢˜
            context: å­¦ä¹ åˆ°çš„çŸ¥è¯†
            genre_config: æ–‡ä½“é…ç½®
        Returns:
            å¤§çº²æ–‡æœ¬
        """
        self._emit_log("ğŸ“‹ [æ‰§ç¬”äºº] æ„å»ºæ·±åº¦å¤§çº²...")

        if genre_config is None:
            genre_config = get_genre_config("é€šç”¨/é»˜è®¤")

        system_role = genre_config.get("system_prompt", "ä¸“ä¸šä½œè€…")
        structure_guide = genre_config.get("structure_guide", "")

        prompt = f"""
        èº«ä»½ï¼š{system_role}
        ç”¨æˆ·è¦æ±‚ï¼š{query}
        å¯ç”¨ç´ æï¼š{context[:5000] if context else "æš‚æ— "}
        ç»“æ„æŒ‡å—ï¼š{structure_guide}

        è¯·æ„å»ºä¸€ä¸ªæ·±åº¦å¤§çº²ï¼ŒåŒ…å«ï¼š
        1. å‰è¨€/å¼•è¨€ï¼ˆæ ¸å¿ƒé—®é¢˜ä¸ä»·å€¼ï¼‰
        2. ç†è®ºæ¡†æ¶ï¼ˆæ ¸å¿ƒæ¦‚å¿µä¸åŸç†ï¼‰
        3. ç°çŠ¶åˆ†æï¼ˆå½“å‰çŠ¶å†µä¸æŒ‘æˆ˜ï¼‰
        4. æ¡ˆä¾‹åˆ†æï¼ˆå…·ä½“å®ä¾‹ä¸æ•°æ®ï¼‰
        5. å¯¹ç­–å»ºè®®ï¼ˆå¯è¡Œæ–¹æ¡ˆä¸æ­¥éª¤ï¼‰
        6. ç»“è®ºå±•æœ›ï¼ˆæ€»ç»“ä¸æœªæ¥æ–¹å‘ï¼‰

        è¯·è¾“å‡ºè¯¦ç»†çš„å¤§çº²ï¼ŒåŒ…å«æ¯ä¸ªéƒ¨åˆ†çš„å…³é”®ç‚¹ã€‚
        """

        try:
            outline = self.llm.chat(prompt, options={"temperature": 0.4, "num_ctx": 8000})
            return outline
        except Exception as e:
            self._emit_log(f"âŒ å¤§çº²æ„å»ºå¤±è´¥: {e}")
            return f"# {query} å¤§çº²\n\n1. å¼•è¨€\n2. ä¸»ä½“\n3. ç»“è®º"

    def write_with_context(self, query, outline, context="", temperature=0.7):
        """
        æ·±åº¦ç ”æŠ¥æ¨¡å¼ä¸“ç”¨ï¼šåŸºäºå¤§çº²å’Œä¸Šä¸‹æ–‡æ’°å†™å†…å®¹

        Args:
            query: ä¸»é¢˜
            outline: å¤§çº²
            context: å­¦ä¹ åˆ°çš„çŸ¥è¯†
            temperature: æ¸©åº¦å‚æ•°
        Returns:
            å®Œæ•´æ–‡ç« 
        """
        self._emit_log("âœï¸ [æ‰§ç¬”äºº] åŸºäºå¤§çº²æ’°å†™æ·±åº¦å†…å®¹...")

        # åˆ†ç« èŠ‚æ’°å†™
        chapters = []

        # 1. å¼•è¨€éƒ¨åˆ†
        intro_prompt = f"""
        åŸºäºä»¥ä¸‹å¤§çº²å’Œç´ æï¼Œæ’°å†™å¼•è¨€éƒ¨åˆ†ï¼š

        ã€ä¸»é¢˜ã€‘ï¼š{query}
        ã€å¤§çº²ã€‘ï¼š{outline}
        ã€å¯ç”¨ç´ æã€‘ï¼š{context[:3000] if context else "æš‚æ— "}

        è¦æ±‚ï¼š
        1. å¸å¼•è¯»è€…æ³¨æ„åŠ›
        2. é˜æ˜é—®é¢˜é‡è¦æ€§
        3. æå‡ºæ ¸å¿ƒè§‚ç‚¹
        4. å­—æ•°çº¦300-500å­—
        """

        try:
            introduction = self.llm.chat(intro_prompt, options={"temperature": temperature, "num_predict": 800})
            chapters.append(f"# å¼•è¨€\n\n{introduction}")

            # 2. ä¸»ä½“éƒ¨åˆ†ï¼ˆåˆ†æ®µè½ï¼‰
            # è¿™é‡Œå¯ä»¥æ›´ç²¾ç»†åœ°æ‹†åˆ†å¤§çº²ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä¸€æ¬¡æ€§æ’°å†™ä¸»ä½“
            body_prompt = f"""
            æ¥ç»­ä»¥ä¸‹å¼•è¨€ï¼Œæ’°å†™æ–‡ç« ä¸»ä½“éƒ¨åˆ†ï¼š

            ã€å¼•è¨€ã€‘ï¼š{introduction[-200:]}
            ã€å®Œæ•´å¤§çº²ã€‘ï¼š{outline}
            ã€å¯ç”¨ç´ æã€‘ï¼š{context[:5000] if context else "æš‚æ— "}

            è¦æ±‚ï¼š
            1. ä¸¥æ ¼éµå¾ªå¤§çº²ç»“æ„
            2. å……åˆ†ä½¿ç”¨ç´ æä¸­çš„æ•°æ®å’Œæ¡ˆä¾‹
            3. é€»è¾‘ä¸¥è°¨ï¼Œè®ºè¯å……åˆ†
            4. å­—æ•°çº¦1500-2000å­—
            """

            body = self.llm.chat(body_prompt, options={"temperature": temperature, "num_predict": 2500})
            chapters.append(f"# ä¸»ä½“\n\n{body}")

            # 3. ç»“è®ºéƒ¨åˆ†
            conclusion_prompt = f"""
            åŸºäºä»¥ä¸‹æ–‡ç« ä¸»ä½“ï¼Œæ’°å†™ç»“è®ºéƒ¨åˆ†ï¼š

            ã€ä¸»é¢˜ã€‘ï¼š{query}
            ã€æ–‡ç« ä¸»ä½“ã€‘ï¼š{body[-500:]}

            è¦æ±‚ï¼š
            1. æ€»ç»“å…¨æ–‡æ ¸å¿ƒè§‚ç‚¹
            2. æå‡ºæœ‰ä»·å€¼çš„å¯ç¤º
            3. å±•æœ›æœªæ¥å‘å±•
            4. å­—æ•°çº¦300-500å­—
            """

            conclusion = self.llm.chat(conclusion_prompt, options={"temperature": temperature, "num_predict": 800})
            chapters.append(f"# ç»“è®º\n\n{conclusion}")

            # ç»„åˆå…¨æ–‡
            full_text = "\n\n".join(chapters)

            # ä¿å­˜å°¾å·´ä¾›ç»­å†™
            self.last_generation_tail = full_text[-1500:]
            self.last_system_role = "æ·±åº¦ç ”æŠ¥ä½œè€…"

            return full_text

        except Exception as e:
            self._emit_log(f"âŒ å†…å®¹æ’°å†™å¤±è´¥: {e}")
            return f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}"

    def global_deep_furnace(self, query, config=None, file_paths=None, callback=None):
        """
        å…¨åŸŸæ·±åº¦ç†”ç‚‰ - å…¨é‡é˜…è¯»+äº‹å®æå–+å¤§çº²æ„å»º+åˆ†ç« æ’°å†™
        """
        config = config or {}

        # è·å–æ–‡ä»¶è·¯å¾„
        if not file_paths:
            file_paths = self._get_input_files()

        if not file_paths:
            return "âŒ æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„æ–‡ä»¶ï¼Œè¯·å…ˆå¯¼å…¥æ–‡æ¡£ã€‚"

        # è·å–æ–‡ä½“é…ç½®
        genre_name = config.get("genre", "é€šç”¨/é»˜è®¤")
        genre_cfg = get_genre_config(genre_name)
        system_role = genre_cfg["system_prompt"]
        structure_guide = genre_cfg["structure_guide"]

        # ä¿å­˜ Role ä¾›ç»­å†™ä½¿ç”¨
        self.last_system_role = system_role

        # é˜¶æ®µä¸€ï¼šå¤šçº¿ç¨‹å¹¶å‘è¯»å–ä¸æå–
        self._emit_log(f"ğŸ”¥ [å…¨åŸŸç†”ç‚‰] å¯åŠ¨ï¼Œå¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")

        # ä»æ–‡ä»¶ä¸­æå–äº‹å®
        from .researcher import Researcher
        researcher = Researcher(
            bus=self.bus,
            llm=self.llm,
            mimicry=self.mimicry,
            analyzer=None,
            io_manager=self.io_manager,
            knowledge_base=self.knowledge_base
        )

        global_facts_pool = researcher.extract_facts_from_files(file_paths, query)

        # é˜¶æ®µäºŒï¼šå…¨åŸŸå¤§çº²æ„å»º
        all_facts_str = "\n".join(global_facts_pool)[:25000]

        outline_prompt = f"""
        èº«ä»½ï¼š{system_role}
        ç”¨æˆ·è¦æ±‚ï¼š{query}
        ç´ æåº“ï¼š{all_facts_str}

        è¯·æ„å»ºã€{genre_name}ã€‘æ·±åº¦å¤§çº²ã€‚
        ç»“æ„æŒ‡å—ï¼š{structure_guide}
        """

        outline = self.llm.chat(outline_prompt, options={"temperature": 0.4, "num_ctx": 8000})

        # ä¿å­˜ä¸Šä¸‹æ–‡
        self.last_global_furnace_context = {
            "query": query,
            "all_facts_str": all_facts_str,
            "outline": outline,
            "genre_name": genre_name,
            "system_role": system_role,
            "structure_guide": structure_guide
        }

        # é˜¶æ®µä¸‰ï¼šåˆ†ç« æ·±åº¦æ’°å†™
        final_parts = [f"# {query}\n\n"]

        # Part 1: å¼€ç¯‡
        p1_prompt = f"èº«ä»½ï¼š{system_role}\nå¤§çº²ï¼š{outline}\nåŸºäºç´ æå†™ç¬¬ä¸€éƒ¨åˆ†(å‰è¨€)ã€‚å­—æ•°1000ã€‚"
        p1 = self.llm.chat(p1_prompt, options={"temperature": 0.6, "num_ctx": 12000})
        final_parts.append(p1)
        self.last_generation_tail = p1[-1500:]

        # Part 2: æ ¸å¿ƒè®ºè¿°
        p2_prompt = f"èº«ä»½ï¼š{system_role}\næ¥ä¸Šæ–‡ï¼š{p1[-500:]}\nå†™ä¸­é—´æ ¸å¿ƒéƒ¨åˆ†ã€‚å­—æ•°1500ã€‚"
        p2 = self.llm.chat(p2_prompt, options={"temperature": 0.6, "num_ctx": 12000})
        final_parts.append(p2)
        self.last_generation_tail = p2[-1500:]

        # Part 3: å…·ä½“ä¸¾æª
        p3_prompt = f"èº«ä»½ï¼š{system_role}\næ¥ä¸Šæ–‡ï¼š{p2[-500:]}\nå†™å…·ä½“çš„ä¸¾æª/ä»»åŠ¡/å¯¹ç­–éƒ¨åˆ†ã€‚å­—æ•°1500ã€‚"
        p3 = self.llm.chat(p3_prompt, options={"temperature": 0.6, "num_ctx": 12000})
        final_parts.append(p3)
        self.last_generation_tail = p3[-1500:]

        # Part 4: ç»“å°¾
        p4_prompt = f"èº«ä»½ï¼š{system_role}\næ¥ä¸Šæ–‡ï¼š{p3[-500:]}\nå†™ç»“è¯­ã€‚å­—æ•°800ã€‚"
        p4 = self.llm.chat(p4_prompt, options={"temperature": 0.7, "num_ctx": 12000})
        final_parts.append(p4)
        self.last_generation_tail = p4[-1500:]

        full_text = "\n\n".join(final_parts)

        # è´¨é‡æ£€æŸ¥å’Œæ¶¦è‰²
        polish_prompt = f"""
        ä½ ç°åœ¨çš„èº«ä»½æ˜¯ï¼š{system_role}

        è¯·å¯¹ä»¥ä¸‹æ–‡ç« è¿›è¡Œæœ€ç»ˆæ¶¦è‰²å’Œæ£€æŸ¥ï¼š

        {full_text}

        æ£€æŸ¥è¦ç‚¹ï¼š
        1. é€»è¾‘è¿è´¯æ€§
        2. äº‹å®å‡†ç¡®æ€§
        3. è¯­è¨€æµç•…åº¦
        4. ç»“æ„å®Œæ•´æ€§
        5. æ˜¯å¦ç¬¦åˆã€{genre_name}ã€‘çš„æ–‡ä½“è¦æ±‚

        è¯·è¾“å‡ºæ¶¦è‰²åçš„å®Œæ•´æ–‡ç« ã€‚
        """

        try:
            polished = self.llm.chat(polish_prompt, options={"temperature": 0.4, "num_ctx": 16000})
            return polished
        except Exception as e:
            self._emit_log(f"âš ï¸ æ¶¦è‰²å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹: {e}")
            return full_text

    def global_report(self, query, config=None):
        """
        é•¿æ–‡ç”Ÿæˆå¼•æ“
        """
        config = config or {}
        user_temp = config.get("temperature", 0.7)

        # è·å–èµ„æ–™åº“å¼•ç”¨ä¿¡æ¯
        file_list = []
        if hasattr(self.knowledge_base, 'get_all_docs'):
            file_list = self.knowledge_base.get_all_docs()
        elif hasattr(self.knowledge_base, 'data'):
            file_list = list(self.knowledge_base.data.get("documents", {}).keys())

        file_list_str = ", ".join(file_list) if file_list else "ç”¨æˆ·ä¸Šä¼ çš„èµ„æ–™åº“"

        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²
        outline_prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„æ”¿ç­–ç ”ç©¶å‘˜ã€‚ç”¨æˆ·è¦æ±‚ï¼š{query}

        ç°æœ‰èµ„æ–™åº“ï¼š[{file_list_str}]

        è¯·åˆ—å‡ºä¸€ä¸ªæ·±åº¦å¤§çº²ã€‚
        âš ï¸ æ ¸å¿ƒè¦æ±‚ï¼š
        1. **å¿…é¡»åŸºäºèµ„æ–™åº“å†…å®¹**ï¼šä¸è¦ç¼–é€ ã€‚
        2. **æ ‡æ³¨æ¥æº**ï¼šåœ¨æ¯ä¸ªç« èŠ‚æ ‡é¢˜åï¼Œç”¨æ‹¬å·æ ‡æ³¨è¯¥ç« èŠ‚ä¸»è¦å‚è€ƒäº†å“ªä¸ªæ–‡ä»¶ã€‚
        3. åŒ…å«ä¸»æ ‡é¢˜ã€å‰¯æ ‡é¢˜ã€‚
        4. è§„åˆ’ 4-5 ä¸ªæ ¸å¿ƒç« èŠ‚ã€‚
        5. æ¯ä¸ªç« èŠ‚ä¸‹åˆ—å‡º 3 ä¸ªå…³é”®ç‚¹ã€‚
        """

        outline = self.llm.chat(outline_prompt, options={"temperature": 0.5})

        # ç¬¬äºŒæ­¥ï¼šåˆ†ç« æ’°å†™
        full_article = [f"# ğŸ“ {query} (ç”Ÿæˆçš„è‰ç¨¿)\n\n"]

        # ç¬¬ä¸€ç« 
        part1_prompt = f"""
        åŸºäºä»¥ä¸‹å¤§çº²ï¼š
        {outline}

        è¯·**åªæ’°å†™ç¬¬ä¸€ç« å’Œå‰è¨€**çš„å†…å®¹ã€‚

        âš ï¸ è¦æ±‚ï¼š
        1. é£æ ¼ï¼š{self.mimicry.generate_system_prompt()}
        2. å­—æ•°ï¼šçº¦ 1000 å­—ã€‚
        3. ä¸è¦å†™åé¢çš„ç« èŠ‚ã€‚
        4. **å¿…é¡»å¼•ç”¨èµ„æ–™åº“ä¸­çš„å†…å®¹**ã€‚
        """
        part1 = self.llm.chat(part1_prompt, options={"temperature": user_temp, "num_predict": 2000})
        full_article.append(part1)

        # ä¸­é—´ç« èŠ‚
        part2_prompt = f"""
        ä¸Šæ–‡å·²å†™ï¼š
        {part1[-500:]}

        è¯·æ¥ç€**æ’°å†™ä¸­é—´çš„æ ¸å¿ƒç« èŠ‚** (ç¬¬äºŒã€ä¸‰ç« )ã€‚

        âš ï¸ ä¸¥å‰çš„æŒ‡ä»¤ï¼š
        1. é€»è¾‘è¡”æ¥ç´§å¯†ï¼Œä¸è¦é‡å¤ä¸Šæ–‡çš„è¯ã€‚
        2. å¼•ç”¨å…·ä½“æ¡ˆä¾‹å’Œæ•°æ®ï¼Œå°½å¯èƒ½å¼•ç”¨èµ„æ–™ä¸­çš„åŸè¯ã€‚
        3. å­—æ•°ï¼šçº¦ 1500 å­—ã€‚
        """
        part2 = self.llm.chat(part2_prompt, options={"temperature": user_temp, "num_predict": 3000})
        full_article.append(part2)

        # ç»“å°¾ç« èŠ‚
        part3_prompt = f"""
        ä¸Šæ–‡å·²å†™ï¼š
        {part2[-500:]}

        è¯·**æ’°å†™æœ€åä¸€ç« å’Œç»“è¯­**ã€‚

        âš ï¸ è¦æ±‚ï¼š
        1. å‡åä¸»é¢˜ï¼Œè¦æœ‰é«˜åº¦ã€‚
        2. å­—æ•°ï¼šçº¦ 800 å­—ã€‚
        3. æ€»ç»“å…¨æ–‡ï¼Œå‘¼åº”å¼€å¤´ã€‚
        """
        part3 = self.llm.chat(part3_prompt, options={"temperature": user_temp, "num_predict": 2000})
        full_article.append(part3)

        full_text = "\n\n".join(full_article)

        # ç¼“å­˜å°¾å·´
        self.last_generation_tail = full_text[-1000:]
        self.last_system_role = "ä¸¥è°¨çš„æ”¿ç­–ç ”ç©¶å‘˜"

        # æ·»åŠ äº¤äº’å¼å¼•å¯¼
        interactive_footer = (
            "\n\n"
            "---"
            "\n> **âš ï¸ ç³»ç»Ÿç›‘æµ‹**ï¼šå¦‚æœä¸Šè¿°å†…å®¹æœªæ˜¾ç¤ºå®Œæ•´ï¼Œæˆ–è€…æ‚¨å¸Œæœ›ç»§ç»­æ‰©å†™ï¼šè¯·ç›´æ¥å›å¤"
            "\n>  ç»§ç»­,æ˜¯æˆ‘å°†æ— ç¼æ¥ç»­ç”Ÿæˆã€‚"
        )

        return full_text + interactive_footer

    def mimicry_write(self, topic, config=None):
        """
        æ‹Ÿæ€ç”Ÿæˆ
        """
        config = config or {}
        prompt = self.mimicry.generate_system_prompt()
        user_prompt = f"è¯·ä»¥è¯¥è§’è‰²çš„å£å»ï¼Œé’ˆå¯¹ä»¥ä¸‹ä¸»é¢˜å‘è¡¨ä¸€æ®µæ·±åˆ»è§è§£ï¼š\nä¸»é¢˜ï¼š{topic}"
        return self.llm.chat(user_prompt, prompt)

    def pyramid_write(self, user_input, config=None):
        """
        é‡‘å­—å¡”å†™ä½œå¼•æ“
        """
        config = config or {}
        audience = config.get("audience", "é€šç”¨è¯»è€…")
        goal = config.get("goal", "ä¼ é€’æ ¸å¿ƒä»·å€¼")
        user_temp = config.get("temperature", 0.7)

        # è·å–èµ„æ–™åº“å¼•ç”¨ä¿¡æ¯
        file_list = []
        if hasattr(self.knowledge_base, 'get_all_docs'):
            file_list = self.knowledge_base.get_all_docs()
        elif hasattr(self.knowledge_base, 'data'):
            file_list = list(self.knowledge_base.data.get("documents", {}).keys())

        file_list_str = ", ".join(file_list) if file_list else "ç”¨æˆ·ä¸Šä¼ çš„èµ„æ–™åº“"

        # ç¬¬ä¸€å±‚ï¼šéª¨æ¶æ¸…æ™°
        structure_prompt = f"""
        ä½ æ˜¯ä¸€ä½é¡¶çº§ç¼–è¾‘ã€‚ç”¨æˆ·æƒ³å†™ï¼š{user_input}

        ã€ç­–ç•¥è®¾å®šã€‘
        - è¯»è€…ç”»åƒï¼š{audience}
        - æ ¸å¿ƒç›®æ ‡ï¼š{goal}

        ã€å¯ç”¨èµ„æ–™åº“ã€‘
        {file_list_str}

        è¯·è®¾è®¡ä¸€ä¸ª**"åŠ¨æ€é€»è¾‘æ¡†æ¶"**ï¼š
        1. **æ ¸å¿ƒè§‚ç‚¹**ï¼šç”¨ä¸€å¥è¯è¯´æ¸…å…¨æ–‡åˆ°åº•è¦è¡¨è¾¾ä»€ä¹ˆã€‚
        2. **å¼€å¤´**ï¼šè®¾è®¡ä¸€ä¸ª"é’©å­"ã€‚
        3. **ä¸»ä½“**ï¼šè®¾è®¡ 3-4 ä¸ªé€»è¾‘å±‚çº§ã€‚
        4. **ç»“å°¾**ï¼šæä¾›"è¡ŒåŠ¨å·å¬"æˆ–"æ„æƒ³ä¸åˆ°çš„æ´è§"ã€‚
        """

        blueprint = self.llm.chat(structure_prompt, options={"temperature": 0.6})

        # ç¬¬äºŒå±‚ï¼šè¡€è‚‰ä¸°æ»¡
        draft_content = []

        # æ’°å†™å¼€å¤´
        intro_prompt = f"""
        åŸºäºæ­¤è®¾è®¡å›¾ï¼š
        {blueprint}

        è¯·æ’°å†™**å¼€å¤´éƒ¨åˆ†**ã€‚
        ã€è¡¨è¾¾è¦æ±‚ã€‘
        - **æ‹’ç»æ¨¡ç³Š**ï¼šå…·ä½“åŒ–ã€‚
        - **å»ºç«‹å…±è¯†**ï¼šå¿«é€Ÿå‘Šè¯‰è¯»è€…"æˆ‘çŸ¥é“ä½ çš„çƒ¦æ¼"ã€‚
        - **å¼•ç”¨èµ„æ–™**ï¼šåŸºäºèµ„æ–™åº“ [{file_list_str}] ä¸­çš„äº‹å®å’Œæ•°æ®ã€‚
        """
        intro = self.llm.chat(intro_prompt, options={"temperature": user_temp})
        draft_content.append(intro)

        # æ’°å†™ä¸»ä½“
        body_prompt = f"""
        åŸºäºè®¾è®¡å›¾ï¼š{blueprint}

        ä¸Šæ–‡å¼€å¤´å·²å†™ï¼š{intro[-300:]}

        è¯·æ’°å†™**æ–‡ç« ä¸»ä½“**ã€‚
        ã€é«˜å¯†åº¦è¦æ±‚ã€‘
        - **ä¿¡æ¯å¯†åº¦**ï¼šå…³é”®å¤„å¿…é¡»æœ‰æ‰å®çš„æ¡ˆä¾‹ã€æ•°æ®æˆ–ç»†èŠ‚ã€‚
        - **å‘¼å¸æ„Ÿ**ï¼šåœ¨è½¬æŠ˜å¤„é€‚å½“ç•™ç™½ï¼Œé•¿çŸ­å¥äº¤æ›¿ã€‚
        - **å…·ä½“åŒ–**ï¼šé¿å…æŠ½è±¡è¡¨è¿°ã€‚
        - **ä¸¥æ ¼å¼•ç”¨**ï¼šåŸºäºèµ„æ–™åº“ [{file_list_str}]ï¼Œç¦æ­¢ç¼–é€ ã€‚
        """
        body = self.llm.chat(body_prompt, options={"temperature": user_temp, "num_predict": 3000})
        draft_content.append(body)

        # æ’°å†™ç»“å°¾
        outro_prompt = f"""
        ä¸Šæ–‡ä¸»ä½“ç»“æŸäºï¼š{body[-300:]}

        è¯·æ’°å†™**ç»“å°¾**ã€‚
        ã€å‡åè¦æ±‚ã€‘
        - **è¶…è¶Šé¢„æœŸ**ï¼šæä¾›ä¸€ä¸ªè§¦åŠ¨äººå¿ƒçš„é‡‘å¥ã€‚
        - **è¡ŒåŠ¨å·å¬**ï¼šç»™å‡ºä¸‹ä¸€æ­¥çš„å…·ä½“å»ºè®®ã€‚
        - **æ€»ç»“å‡å**ï¼šå‘¼åº”å¼€å¤´ï¼Œå¼ºåŒ–æ ¸å¿ƒè§‚ç‚¹ã€‚
        """
        outro = self.llm.chat(outro_prompt, options={"temperature": user_temp})
        draft_content.append(outro)

        full_draft = "\n\n".join(draft_content)

        # ç¬¬ä¸‰å±‚ï¼šåŒ å¿ƒæ‰“ç£¨
        editor_prompt = f"""
        ä½ æ˜¯ä¸€ä½æå…¶æŒ‘å‰”çš„ä¸»ç¼–ã€‚è¿™æ˜¯åˆšå†™å¥½çš„åˆç¨¿ï¼š

        {full_draft}

        è¯·å¯¹ç…§ä»¥ä¸‹**ã€å¥½ç¨¿å­æ ¸å¯¹æ¸…å•ã€‘**è¿›è¡Œæ¶¦è‰²å’Œä¿®å‰ªï¼š
        1. **ç®€æ´**ï¼šåˆ é™¤ä¸€åˆ‡ä¸"{goal}"æ— å…³çš„åºŸè¯ã€‚
        2. **çœŸè¯š**ï¼šå»æ‰æ•…å¼„ç„è™šå’Œå¤¸å¤§å…¶è¯ã€‚
        3. **èŠ‚å¥**ï¼šå¤§å£°æœ—è¯»ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼Œä¿®æ”¹æ‹—å£çš„åœ°æ–¹ã€‚
        4. **ä»·å€¼**ï¼šè¯»è€…èƒ½åœ¨3ç§’å†…æ˜ç™½æ­¤æ–‡ä»·å€¼å—ï¼Ÿ
        5. **å¼•ç”¨éªŒè¯**ï¼šæ£€æŸ¥æ‰€æœ‰å¼•ç”¨æ˜¯å¦å‡†ç¡®ã€‚

        è¯·è¾“å‡º**æœ€ç»ˆå®šç¨¿**ã€‚
        """

        final_article = self.llm.chat(editor_prompt, options={"temperature": 0.4, "num_ctx": 12000})

        # ç¼“å­˜å°¾å·´
        self.last_generation_tail = final_article[-1000:]
        self.last_system_role = "é¡¶çº§ç¼–è¾‘/ä¸»ç¼–"

        return final_article

    def _get_input_files(self):
        """è·å–è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        file_paths = []
        if hasattr(self.io_manager, 'paths'):
            inputs_dir = self.io_manager.paths.directories.get('inputs')
            if inputs_dir and os.path.exists(inputs_dir):
                file_paths = [os.path.join(inputs_dir, f) for f in os.listdir(inputs_dir)
                              if not f.startswith("~$") and os.path.isfile(os.path.join(inputs_dir, f))]
        return file_paths

    # æä¾›ç»Ÿä¸€çš„å†™ä½œæ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å¼
    def write(self, topic: str, mode: str = "intelligent", **kwargs) -> str:
        """
        ç»Ÿä¸€å†™ä½œæ¥å£

        Args:
            topic: å†™ä½œä¸»é¢˜
            mode: å†™ä½œæ¨¡å¼
                - "intelligent": æ™ºèƒ½å†™ä½œï¼ˆé»˜è®¤ï¼‰
                - "continuation": ç»­å†™
                - "global_furnace": å…¨åŸŸæ·±åº¦ç†”ç‚‰
                - "global_report": é•¿æ–‡ç”Ÿæˆ
                - "mimicry": æ‹Ÿæ€ç”Ÿæˆ
                - "pyramid": é‡‘å­—å¡”å†™ä½œ
                - "deep_research": æ·±åº¦ç ”æŠ¥
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if mode == "intelligent":
            strategy = kwargs.get("strategy", {})
            return self.intelligent_write(topic, strategy)
        elif mode == "continuation":
            tail = kwargs.get("tail", self.last_generation_tail)
            role = kwargs.get("role", self.last_system_role)
            return self.handle_continuation(tail, role)
        elif mode == "global_furnace":
            config = kwargs.get("config", {})
            file_paths = kwargs.get("file_paths", None)
            return self.global_deep_furnace(topic, config, file_paths)
        elif mode == "global_report":
            config = kwargs.get("config", {})
            return self.global_report(topic, config)
        elif mode == "mimicry":
            config = kwargs.get("config", {})
            return self.mimicry_write(topic, config)
        elif mode == "pyramid":
            config = kwargs.get("config", {})
            return self.pyramid_write(topic, config)
        elif mode == "deep_research":
            context = kwargs.get("context", "")
            genre_config = kwargs.get("genre_config", None)
            outline = self.create_outline(topic, context, genre_config)
            temperature = kwargs.get("temperature", 0.7)
            return self.write_with_context(topic, outline, context, temperature)
        else:
            self._emit_log(f"âš ï¸ æœªçŸ¥å†™ä½œæ¨¡å¼: {mode}ï¼Œä½¿ç”¨æ™ºèƒ½å†™ä½œæ¨¡å¼")
            strategy = kwargs.get("strategy", {})
            return self.intelligent_write(topic, strategy)