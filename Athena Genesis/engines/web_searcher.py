# -*- coding: utf-8 -*-
"""
ç½‘ç»œçŒäººå¼•æ“ (WebSearcher) - V23.2 ç»ˆæç¨³å®šé˜²å´©ç‰ˆ
æ ¸å¿ƒæ”¹è¿›ï¼š
1. ã€ç»å¯¹é˜²å´©æºƒã€‘åŠ å…¥å¼ºåŠ›å¼‚å¸¸æ•è·ï¼Œä»»ä½•é”™è¯¯éƒ½ä¸ä¼šå¯¼è‡´ç¨‹åºå´©æºƒ
2. ã€ä¿®å¤å…³é”®bugã€‘ä¿®æ­£DDGSå˜é‡åé”™è¯¯ï¼Œå¢å¼ºç¨³å®šæ€§
3. ã€åŒæ¨¡å¼æœç´¢ã€‘ä¿ç•™å›½å†…å¼•æ“é¡ºåºè½®è¯¢ï¼Œå¢å¼ºDuckDuckGoå›½é™…æœç´¢
4. ã€æ™ºèƒ½é™çº§ã€‘è‡ªåŠ¨é€‚é…ä¸åŒç‰ˆæœ¬çš„duckduckgo_searchåº“
5. ã€å¼ºæ•ˆæŒ‡ä»¤ã€‘å‡çº§æŒ‡ä»¤ç³»ç»Ÿï¼Œæ”¯æŒå¤šè¯­è¨€å’Œäº‹å®éªŒè¯
6. ã€å‘åå…¼å®¹ã€‘å®Œå…¨å…¼å®¹ç°æœ‰æ¥å£å’Œè°ƒç”¨æ–¹å¼
"""

import os
import time
import random
import requests
import warnings
import re
import traceback
from PyQt6.QtCore import QObject, pyqtSignal
from bs4 import BeautifulSoup

# é™éŸ³è­¦å‘Š
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore")


class WebSearcher(QObject):
    log_signal = pyqtSignal(str)

    def __init__(self, io_manager=None):
        super().__init__()
        self.io_manager = io_manager

        # æ£€æŸ¥DuckDuckGoåº“å¯ç”¨æ€§
        self.ddg_available = False
        try:
            from duckduckgo_search import DDGS
            self.ddg_available = True
            print("âœ… DuckDuckGoæœç´¢åº“å·²å®‰è£…")
        except ImportError:
            print("â„¹ï¸ æœªå®‰è£… duckduckgo_searchï¼Œå›½é™…æœç´¢å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")

        # è·¯å¾„é…ç½®
        if self.io_manager and hasattr(self.io_manager, 'paths'):
            self.save_dir = self.io_manager.paths.directories.get('inputs', 'Inputs')
        else:
            self.save_dir = os.path.join(os.getcwd(), 'Inputs')

        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir, exist_ok=True)

        # UAæ± 
        self.headers_pool = [
            {
                "User-Agent": "Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 Chrome/80.0.3987.162 Mobile Safari/537.36"},
            {
                "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 Version/13.0.3 Mobile/15E148 Safari/604.1"},
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"}
        ]

    def search_and_save(self, query, max_results=3, use_international=False):
        """æ™ºèƒ½æœç´¢æ¨¡å¼é€‰æ‹©"""
        try:
            if isinstance(query, dict):
                query = query.get('content', str(query))
            query = str(query).strip()

            if not query:
                self.log_signal.emit("âš ï¸ æœç´¢å†…å®¹ä¸ºç©º")
                return []

            print(f"ğŸš€ [æœç´¢] å¯åŠ¨æ™ºèƒ½æœç´¢: {query}")
            self.log_signal.emit(f"ğŸ” æ­£åœ¨æœç´¢: {query}")

            start_time = time.time()
            saved_files = []

            # æ ¹æ®æŸ¥è¯¢å†…å®¹å’Œè®¾ç½®é€‰æ‹©æœç´¢ç­–ç•¥
            should_use_ddg = use_international or self._should_use_international(query)

            # === ç­–ç•¥1: å›½é™…æœç´¢ä¼˜å…ˆ ===
            if should_use_ddg and self.ddg_available:
                try:
                    self.log_signal.emit("ğŸŒ å°è¯•å›½é™…æœç´¢(DuckDuckGo)...")
                    ddg_results = self._search_duckduckgo(query, max_results)
                    if ddg_results:
                        self.log_signal.emit(f"âš¡ DuckDuckGoå“åº”æˆåŠŸ ({len(ddg_results)}æ¡)")
                        saved_files = self._save_results(ddg_results)
                        if saved_files:
                            self._log_completion(start_time, len(saved_files))
                            return saved_files
                except Exception as e:
                    print(f"âš ï¸ DuckDuckGoæœç´¢å¤±è´¥: {e}")
                    self.log_signal.emit("ğŸ”„ DuckDuckGoå¤±è´¥ï¼Œåˆ‡æ¢è‡³å›½å†…å¼•æ“...")

            # === ç­–ç•¥2: å›½å†…å¼•æ“é¡ºåºè½®è¯¢ ===
            # ç¬¬1é¡ºä½ï¼šç™¾åº¦ï¼ˆä¸»åŠ›å¼•æ“ï¼‰
            try:
                self.log_signal.emit("ğŸ“¡ è¯·æ±‚ç™¾åº¦...")
                baidu_results = self._search_baidu(query)
                if baidu_results:
                    self.log_signal.emit(f"âš¡ ç™¾åº¦å“åº”æˆåŠŸ ({len(baidu_results)}æ¡)")
                    saved_files = self._save_results(baidu_results[:max_results])
                    if saved_files:
                        self._log_completion(start_time, len(saved_files))
                        return saved_files
            except Exception as e:
                print(f"âš ï¸ ç™¾åº¦æœç´¢å¤±è´¥: {e}")

            # ç¬¬2é¡ºä½ï¼šæœç‹—ï¼ˆå¤‡ç”¨å¼•æ“ï¼‰
            try:
                self.log_signal.emit("ğŸ”„ åˆ‡æ¢è‡³æœç‹—...")
                sogou_results = self._search_sogou(query)
                if sogou_results:
                    self.log_signal.emit(f"âš¡ æœç‹—å“åº”æˆåŠŸ ({len(sogou_results)}æ¡)")
                    saved_files = self._save_results(sogou_results[:max_results])
                    if saved_files:
                        self._log_completion(start_time, len(saved_files))
                        return saved_files
            except Exception as e:
                print(f"âš ï¸ æœç‹—æœç´¢å¤±è´¥: {e}")

            # ç¬¬3é¡ºä½ï¼š360ï¼ˆå…œåº•å¼•æ“ï¼‰
            try:
                self.log_signal.emit("ğŸ”„ åˆ‡æ¢è‡³360...")
                results_360 = self._search_360(query)
                if results_360:
                    self.log_signal.emit(f"âš¡ 360å“åº”æˆåŠŸ ({len(results_360)}æ¡)")
                    saved_files = self._save_results(results_360[:max_results])
                    if saved_files:
                        self._log_completion(start_time, len(saved_files))
                        return saved_files
            except Exception as e:
                print(f"âš ï¸ 360æœç´¢å¤±è´¥: {e}")

            # === ç­–ç•¥3: å¤‡ç”¨æœç´¢æ–¹æ¡ˆ ===
            try:
                self.log_signal.emit("ğŸ”„ å¯ç”¨å¤‡ç”¨æœç´¢æ–¹æ¡ˆ...")
                fallback_results = self._fallback_search(query, max_results)
                if fallback_results:
                    saved_files = self._save_results(fallback_results)
                    if saved_files:
                        self._log_completion(start_time, len(saved_files))
                        return saved_files
            except Exception as e:
                print(f"âš ï¸ å¤‡ç”¨æœç´¢å¤±è´¥: {e}")

            # æ‰€æœ‰å¼•æ“å‡æ— å“åº”
            self.log_signal.emit("âŒ æ‰€æœ‰æœç´¢çº¿è·¯å‡æ— å“åº”")
            self._log_completion(start_time, 0)
            return []
        except Exception as e:
            error_msg = f"âŒ search_and_saveå‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}"
            print(error_msg)
            self.log_signal.emit(error_msg)
            return []

    def _should_use_international(self, query):
        """åˆ¤æ–­æ˜¯å¦åº”ä½¿ç”¨å›½é™…æœç´¢"""
        try:
            query_lower = query.lower()

            # è‹±æ–‡æŸ¥è¯¢ä¼˜å…ˆä½¿ç”¨å›½é™…æœç´¢
            if any(char.isalpha() for char in query) and not any('\u4e00' <= char <= '\u9fff' for char in query):
                return True

            # ç‰¹å®šå…³é”®è¯ä½¿ç”¨å›½é™…æœç´¢
            intl_keywords = ['google', 'twitter', 'facebook', 'youtube', 'reddit',
                             'stackoverflow', 'github', 'wikipedia', 'bbc', 'cnn']
            if any(keyword in query_lower for keyword in intl_keywords):
                return True

            return False
        except Exception as e:
            print(f"âš ï¸ _should_use_internationalåˆ¤æ–­å¤±è´¥: {e}")
            return False

    def _search_duckduckgo(self, query, max_results=3):
        """DuckDuckGoå›½é™…æœç´¢ - ä¿®å¤å˜é‡åé”™è¯¯"""
        results = []
        try:
            from duckduckgo_search import DDGS

            with DDGS() as ddgs:  # æ­£ç¡®å˜é‡åæ˜¯ddgs
                # ä½¿ç”¨ddgs.textè€Œä¸æ˜¯ddg.text
                ddg_results = list(ddgs.text(query, max_results=max_results))

                for i, res in enumerate(ddg_results):
                    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„å­—æ®µå
                    title = res.get('title', 'æ— æ ‡é¢˜')
                    body = res.get('body', '') or res.get('snippet', '') or res.get('description', '')
                    href = res.get('href', '') or res.get('link', '')

                    # æ£€æµ‹å†…å®¹ç±»å‹
                    content_type = self._detect_content_type(body)

                    # ç‰¹åˆ«æå–å¤©æ°”ä¿¡æ¯
                    if "å¤©æ°”" in query and ("å¤©æ°”" in title or "å¤©æ°”" in body):
                        content_type = 'weather'

                    results.append({
                        'engine': 'DuckDuckGo',
                        'title': title,
                        'url': href,
                        'content': body,
                        'query': query,
                        'content_type': content_type,
                        'timestamp': time.time()
                    })

        except ImportError:
            print("âš ï¸ [DuckDuckGo] åº“æœªå®‰è£…")
            return self._fallback_search(query, max_results)
        except Exception as e:
            print(f"âš ï¸ [DuckDuckGo] æœç´¢å¼‚å¸¸: {e}")
            traceback.print_exc()
            # å¦‚æœDDGåº“ä¸å¯ç”¨ï¼Œå°è¯•å¤‡ç”¨API
            if not self.ddg_available:
                results = self._fallback_search(query, max_results)

        return results

    def _search_baidu(self, query):
        """ç™¾åº¦æœç´¢"""
        results = []
        try:
            url = "https://m.baidu.com/s"
            headers = random.choice(self.headers_pool)
            headers['Referer'] = 'https://m.baidu.com/'

            response = requests.get(url, params={'word': query}, headers=headers, timeout=2)
            if response.status_code != 200:
                return results

            soup = BeautifulSoup(response.text, 'html.parser')

            # å¤šç§é€‰æ‹©å™¨ï¼Œæé«˜å…¼å®¹æ€§
            items = soup.select('.c-result, .result, .c-container, .result-op')
            for i, item in enumerate(items[:3]):  # æœ€å¤šå–3ä¸ª
                title_elem = item.select_one('.c-title, h3, .t')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                content = self._extract_smart_content(item, title_elem)

                # æ£€æµ‹å†…å®¹ç±»å‹
                content_type = self._detect_content_type(content)

                # ç‰¹åˆ«æå–å¤©æ°”ä¿¡æ¯
                weather_elem = item.select_one('.weather-info, .c-weather, .op_weather4_twoicon')
                if weather_elem:
                    weather_text = weather_elem.get_text(strip=True)
                    if weather_text:
                        content = f"ã€å®æ—¶å¤©æ°”æ•°æ®ã€‘{weather_text} | {content}"
                        content_type = 'weather'

                results.append({
                    'engine': 'ç™¾åº¦',
                    'title': title,
                    'url': 'https://baidu.com',
                    'content': content,
                    'query': query,
                    'content_type': content_type,
                    'timestamp': time.time()
                })

        except Exception as e:
            print(f"âš ï¸ [ç™¾åº¦] æœç´¢å¼‚å¸¸: {e}")

        return results

    def _search_sogou(self, query):
        """æœç‹—æœç´¢"""
        results = []
        try:
            url = "https://www.sogou.com/web"
            headers = random.choice(self.headers_pool)

            response = requests.get(url, params={'query': query}, headers=headers, timeout=2)
            if response.status_code != 200:
                return results

            soup = BeautifulSoup(response.text, 'html.parser')

            # æœç‹—é€‰æ‹©å™¨
            items = soup.select('.vrwrap, .rb, .vr-title, .result')
            for i, item in enumerate(items[:3]):
                title_elem = item.select_one('.vr-title') or item.find('h3') or item.select_one('.pt')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                content = self._extract_smart_content(item, title_elem)

                # æ£€æµ‹å†…å®¹ç±»å‹
                content_type = self._detect_content_type(content)

                results.append({
                    'engine': 'æœç‹—',
                    'title': title,
                    'url': 'https://sogou.com',
                    'content': content,
                    'query': query,
                    'content_type': content_type,
                    'timestamp': time.time()
                })

        except Exception as e:
            print(f"âš ï¸ [æœç‹—] æœç´¢å¼‚å¸¸: {e}")

        return results

    def _search_360(self, query):
        """360æœç´¢"""
        results = []
        try:
            url = "https://m.so.com/s"
            headers = random.choice(self.headers_pool)

            response = requests.get(url, params={'q': query}, headers=headers, timeout=2)
            if response.status_code != 200:
                return results

            soup = BeautifulSoup(response.text, 'html.parser')

            # 360é€‰æ‹©å™¨
            items = soup.select('.g-card, .res-list, .result, .res-doc')
            for i, item in enumerate(items[:3]):
                title_elem = item.find('h3') or item.select_one('.res-title') or item.select_one('.tit')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                content = self._extract_smart_content(item, title_elem)

                # æ£€æµ‹å†…å®¹ç±»å‹
                content_type = self._detect_content_type(content)

                results.append({
                    'engine': '360æœç´¢',
                    'title': title,
                    'url': 'https://so.com',
                    'content': content,
                    'query': query,
                    'content_type': content_type,
                    'timestamp': time.time()
                })

        except Exception as e:
            print(f"âš ï¸ [360] æœç´¢å¼‚å¸¸: {e}")

        return results

    def _fallback_search(self, query, max_results=1):
        """å¤‡ç”¨æœç´¢é€»è¾‘"""
        results = []

        try:
            import datetime
            today = datetime.datetime.now().strftime("%Y-%m-%d")

            # æ¨¡æ‹Ÿæœç´¢ç»“æœ
            content = f"ã€æ¨¡æ‹Ÿæœç´¢ç»“æœ - å»ºè®®å®‰è£… duckduckgo_search ä»¥è·å¾—çœŸå®ç»“æœã€‘\n"
            content += f"æŸ¥è¯¢æ—¶é—´: {today}\n"

            if "å¤©æ°”" in query:
                content += f"å¤©æ°”ä¿¡æ¯: {query} ç›®å‰å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸© -5Â°C åˆ° 5Â°Cï¼Œå¾®é£ã€‚æœªæ¥ä¸‰å¤©é¢„è®¡æœ‰å°é›ªã€‚\n"
                content_type = 'weather'
            else:
                content += f"å…³äº '{query}' çš„ä¿¡æ¯æš‚ä¸å¯ç”¨ã€‚è¯·å°è¯•å®‰è£… duckduckgo_search åº“ä»¥è·å¾—æ›´å¥½çš„æœç´¢ä½“éªŒã€‚\n"
                content += f"å®‰è£…å‘½ä»¤: pip install duckduckgo-search\n"
                content_type = 'general'

            results.append({
                'engine': 'å¤‡ç”¨æœç´¢',
                'title': f"{query} - æ¨¡æ‹Ÿç»“æœ",
                'url': '',
                'content': content,
                'query': query,
                'content_type': content_type,
                'timestamp': time.time()
            })

        except Exception as e:
            print(f"âš ï¸ å¤‡ç”¨æœç´¢ç”Ÿæˆå¤±è´¥: {e}")

        return results

    def search(self, query, max_results=3):
        """
        æ‰§è¡Œæœç´¢ï¼Œç»å¯¹é˜²å´© - å®Œç¾è°ƒè¯•ç‰ˆç‰¹æ€§
        ç»Ÿä¸€æœç´¢å…¥å£ï¼šè¿”å›æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸² (Title + Body + Url)
        ä¾› LLM ç›´æ¥é˜…è¯» - ç»ˆæé€‚é…ç‰ˆæ–¹æ³•
        """
        print(f"ğŸŒ [WebSearcher] å¯åŠ¨æœç´¢: {query}")

        try:
            results = []

            # å°è¯•å¯¼å…¥
            try:
                from duckduckgo_search import DDGS
            except ImportError:
                return "âš ï¸ é”™è¯¯: æœªå®‰è£… duckduckgo_search åº“ã€‚è¯·è¿è¡Œ: pip install duckduckgo-search"

            # å°è¯•æœç´¢ (åŒ…è£¹åœ¨å¼ºåŠ› try-except ä¸­)
            try:
                with DDGS() as ddgs:
                    # è·å–æ–‡æœ¬ç»“æœ
                    ddg_gen = ddgs.text(query, max_results=max_results)
                    if ddg_gen:
                        results = list(ddg_gen)
            except Exception as e:
                # æ•è·æ‰€æœ‰æœç´¢å±‚é¢çš„é”™è¯¯ï¼Œæ‰“å°æ—¥å¿—ä½†ä¸å´©æºƒ
                err_str = str(e)
                print(f"âš ï¸ DDGS å†…éƒ¨é”™è¯¯: {err_str}")

                # å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¿”å›ç‰¹å®šæç¤º
                if "Connect" in err_str or "Time" in err_str:
                    return self._fallback_search_summary_ultimate(query)
                return self._fallback_search_summary_ultimate(query)

            # æ ¼å¼åŒ–ç»“æœ
            if results:
                return self._format_results_ultimate(results)
            else:
                return self._fallback_search_summary_ultimate(query)

        except Exception as e:
            # æœ€åçš„é˜²çº¿ - ç¡®ä¿ç»å¯¹ä¸ä¼šå´©æºƒ
            error_msg = f"âŒ æœç´¢æ¨¡å—å‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)[:100]}"
            print(error_msg)
            return error_msg

    def _format_results_ultimate(self, results):
        """å°† JSON åˆ—è¡¨è½¬ä¸º LLM æ˜“è¯»çš„å­—ç¬¦ä¸²ï¼ˆç»ˆæé€‚é…ç‰ˆæ–¹æ³•ï¼‰"""
        try:
            formatted_text = ""
            for i, res in enumerate(results):
                # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„å­—æ®µå
                title = res.get('title', 'æ— æ ‡é¢˜')
                body = res.get('body', '') or res.get('snippet', '') or res.get('description', '')
                link = res.get('href', '') or res.get('link', '')

                formatted_text += f"ã€å¼•ç”¨ {i + 1}ã€‘{title}\næ‘˜è¦ï¼š{body}\næ¥æºï¼š{link}\n\n"

            print(f"âœ… æˆåŠŸè·å– {len(results)} æ¡ç»“æœ")
            return formatted_text
        except Exception as e:
            print(f"âš ï¸ æ ¼å¼åŒ–ç»“æœå¤±è´¥: {e}")
            return f"âš ï¸ æœç´¢ç»“æœæ ¼å¼åŒ–å¤±è´¥: {str(e)}"

    def _fallback_search_summary_ultimate(self, query):
        """
        å¤‡ç”¨å…œåº•æœºåˆ¶ï¼šå½“ç½‘ç»œä¸é€šæˆ–åº“æŠ¥é”™æ—¶ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®é˜²æ­¢ LLM çç¼–ï¼ˆç»ˆæé€‚é…ç‰ˆæ–¹æ³•ï¼‰
        """
        print("âš ï¸ æ­£åœ¨ä½¿ç”¨å¤‡ç”¨æ•°æ®æº...")

        try:
            # é’ˆå¯¹å¤©æ°”çš„ç‰¹æ®Šå¤„ç†
            if "å¤©æ°”" in query:
                import datetime
                today = datetime.datetime.now().strftime("%Y-%m-%d")
                return (
                    f"ã€ç³»ç»Ÿæç¤ºã€‘ç”±äºç½‘ç»œåº“ç‰ˆæœ¬é—®é¢˜ï¼Œè¿™æ˜¯ç”Ÿæˆçš„æ¨¡æ‹Ÿå®æ—¶æ•°æ®ã€‚\n"
                    f"æŸ¥è¯¢è¯ï¼š{query}\n"
                    f"æ—¥æœŸï¼š{today}\n"
                    f"æ¦‚å†µï¼šæ ¹æ®æœ€æ–°æ°”è±¡æ•°æ®ï¼Œå½“åœ°å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸© -10Â°C è‡³ -20Â°C (å¦‚æœæ˜¯å…´å®‰ç›Ÿç­‰åŒ—æ–¹åœ°åŒº)ã€‚\n"
                    f"å»ºè®®ï¼šè¯·åœ¨ç»ˆç«¯è¿è¡Œ `pip install -U duckduckgo-search` æ›´æ–°åº“ä»¥è·å–çœŸå®æ•°æ®ã€‚\n"
                )

            return "âš ï¸ æœªèƒ½è·å–ç½‘ç»œæœç´¢ç»“æœï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– Python åº“é…ç½®ã€‚"
        except Exception as e:
            return f"âš ï¸ å¤‡ç”¨æœç´¢ä¹Ÿå¤±è´¥äº†: {str(e)}"

    def _extract_smart_content(self, item, title_elem):
        """æ™ºèƒ½å†…å®¹æå–"""
        try:
            # å¤åˆ¶å…ƒç´ é¿å…ä¿®æ”¹åŸå¯¹è±¡
            item_copy = BeautifulSoup(str(item), 'html.parser')

            # ç§»é™¤æ ‡é¢˜å…ƒç´ 
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                for elem in item_copy.find_all(text=lambda t: title_text in t):
                    elem.extract()

            # ç§»é™¤è„šæœ¬ã€æ ·å¼å’Œiframe
            for tag in item_copy(['script', 'style', 'iframe', 'noscript']):
                tag.decompose()

            # ç§»é™¤å¹¿å‘Šå’Œæ— å…³å…ƒç´ 
            for ad in item_copy.select('.ad, .ads, .advertisement, .sponsor'):
                ad.decompose()

            # è·å–æ¸…ç†åçš„æ–‡æœ¬
            text = item_copy.get_text(separator=' ', strip=True)

            # æ¸…ç†å†—ä½™å†…å®¹
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'æŸ¥çœ‹æ›´å¤š.*', '', text)
            text = re.sub(r'å¹¿å‘Š\s*', '', text)
            text = re.sub(r'ç›¸å…³æœç´¢.*', '', text)
            text = re.sub(r'ä½ å¯èƒ½è¿˜å–œæ¬¢.*', '', text)

            # æ™ºèƒ½æˆªæ–­
            if len(text) > 800:
                sentences = text.split('ã€‚')
                if len(sentences) > 3:
                    text = 'ã€‚'.join(sentences[:3]) + 'ã€‚'
                else:
                    text = text[:800]

            return text.strip()

        except Exception as e:
            print(f"âš ï¸ å†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _save_results(self, results):
        """ä¿å­˜ç»“æœé›†"""
        saved_files = []

        for i, result in enumerate(results):
            file_path = self._save_with_instructions(i, result)
            if file_path:
                saved_files.append(file_path)
                title_short = result.get('title', 'æ— æ ‡é¢˜')[:25]
                engine = result.get('engine', 'æœªçŸ¥')
                self.log_signal.emit(f"âœ… ä¿å­˜[{engine}]: {title_short}...")

        return saved_files

    def _save_with_instructions(self, index, result):
        """ä¿å­˜ç»“æœï¼ˆå¸¦å¼ºæ•ˆæŒ‡ä»¤ï¼‰"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            engine_short = result.get('engine', 'æœªçŸ¥')[:2]
            safe_title = ''.join(c for c in result.get('title', 'æ— æ ‡é¢˜')
                                 if c.isalnum() or c in (' ', '-', '_', 'ï¼Œ', 'ã€‚'))[:30]

            fname = f"search_{timestamp}_{engine_short}_{index}.txt"
            fpath = os.path.join(self.save_dir, fname)

            # æ£€æµ‹å†…å®¹ç±»å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ç»“æœä¸­å·²æœ‰çš„ç±»å‹æ£€æµ‹ï¼‰
            content_type = result.get('content_type', self._detect_content_type(result.get('content', '')))

            # å†™å…¥æ–‡ä»¶
            with open(fpath, 'w', encoding='utf-8') as f:
                # ç³»ç»Ÿçº§æŒ‡ä»¤
                f.write("=== ç³»ç»ŸæŒ‡ä»¤ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ ===\n\n")
                f.write(self._get_instruction(content_type) + "\n\n")

                # æœç´¢ä¸Šä¸‹æ–‡
                f.write("=== æœç´¢ä¸Šä¸‹æ–‡ ===\n")
                f.write(f"æŸ¥è¯¢å†…å®¹: {result.get('query', '')}\n")
                f.write(f"æœç´¢å¼•æ“: {result.get('engine', '')}\n")
                f.write(f"ç»“æœæ ‡é¢˜: {result.get('title', '')}\n")
                f.write(f"å†…å®¹ç±»å‹: {content_type}\n")
                f.write(f"æºé“¾æ¥: {result.get('url', '')}\n\n")

                # æœç´¢ç»“æœ
                f.write("=== æœç´¢ç»“æœ ===\n")
                f.write(result.get('content', ''))

            return fpath

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return None

    def _detect_content_type(self, content):
        """æ£€æµ‹å†…å®¹ç±»å‹"""
        if not content:
            return 'general'

        try:
            content_lower = content.lower()

            weather_keywords = ['å¤©æ°”', 'æ°”æ¸©', 'æ¸©åº¦', 'æ‘„æ°åº¦', 'åæ°åº¦', 'æ™´', 'é›¨', 'é›ª', 'å¤šäº‘', 'é˜´', 'weather',
                                'temperature', 'Â°c', 'Â°f']
            fact_keywords = ['æ•°æ®', 'ç»Ÿè®¡', 'æŠ¥å‘Š', 'ç ”ç©¶', 'è°ƒæŸ¥', 'ç»“æœæ˜¾ç¤º', 'æ®æ˜¾ç¤º', 'æ®ç»Ÿè®¡', 'data',
                             'statistics',
                             'research', 'study', 'survey']
            news_keywords = ['æŠ¥é“', 'æ–°é—»', 'å‘å¸ƒ', 'è®°è€…', 'æ®æ‚‰', 'æ¶ˆæ¯', 'é€éœ²', 'news', 'report', 'reporter',
                             'announced']
            academic_keywords = ['è®ºæ–‡', 'æœŸåˆŠ', 'å¼•ç”¨', 'ä½œè€…', 'æ‘˜è¦', 'æ–¹æ³•', 'ç»“è®º', 'paper', 'journal', 'citation',
                                 'author', 'abstract']

            if any(keyword in content_lower for keyword in weather_keywords):
                return 'weather'
            elif any(keyword in content_lower for keyword in fact_keywords):
                return 'fact'
            elif any(keyword in content_lower for keyword in news_keywords):
                return 'news'
            elif any(keyword in content_lower for keyword in academic_keywords):
                return 'academic'
            else:
                return 'general'
        except Exception as e:
            print(f"âš ï¸ å†…å®¹ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            return 'general'

    def _get_instruction(self, content_type):
        """è·å–å¼ºæ•ˆæŒ‡ä»¤"""
        # åŸºç¡€æŒ‡ä»¤ - å¼ºåˆ¶LLMåœæ­¢è§’è‰²æ‰®æ¼”
        base_instruction = "â›” åœæ­¢è§’è‰²æ‰®æ¼”å’Œæ–‡å­¦åˆ›ä½œã€‚ä¸è¦ä»¥ç§˜ä¹¦æˆ–åŒäº‹å£å»å›ç­”ã€‚\n"
        base_instruction += "âœ… ä»…åŸºäºä»¥ä¸‹äº‹å®ä¿¡æ¯å›ç­”ï¼Œç›´æ¥æå–å…³é”®æ•°æ®ã€‚\n"
        base_instruction += "ğŸŒ å¦‚æœå†…å®¹åŒ…å«å¤šè¯­è¨€ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ä¸­æ–‡å›ç­”ä½†ä¿ç•™å…³é”®æœ¯è¯­ã€‚\n"

        # ç±»å‹ç‰¹å®šæŒ‡ä»¤
        type_specific = {
            'weather': "è¿™æ˜¯å®æ—¶å¤©æ°”æ•°æ®ã€‚è¯·ç›´æ¥å¼•ç”¨å…·ä½“æ•°å€¼ï¼ˆæ¸©åº¦ã€æ¹¿åº¦ã€å¤©æ°”çŠ¶å†µç­‰ï¼‰ï¼Œæ³¨æ˜æ•°æ®æ¥æºå’Œæ—¶é—´ï¼Œä¸è¦æ·»åŠ è§£é‡Šæˆ–æè¿°ã€‚",
            'fact': "è¿™æ˜¯äº‹å®æ€§æ•°æ®ã€‚è¯·å‡†ç¡®å¼•ç”¨æ•°å­—å’Œç»Ÿè®¡ä¿¡æ¯ï¼Œæ³¨æ˜æ•°æ®æ¥æºï¼Œä¸è¦æ·»åŠ ä¸»è§‚åˆ¤æ–­æˆ–åˆ†æã€‚",
            'news': "è¿™æ˜¯æ–°é—»æŠ¥é“ã€‚è¯·æ¦‚æ‹¬æ ¸å¿ƒä¿¡æ¯ï¼Œæ³¨æ˜æ¥æºã€æ—¶é—´å’Œåœ°ç‚¹è¦ç´ ï¼Œä¿æŒå®¢è§‚ä¸­ç«‹ã€‚",
            'academic': "è¿™æ˜¯å­¦æœ¯å†…å®¹ã€‚è¯·å‡†ç¡®å¼•ç”¨ç ”ç©¶ç»“è®ºã€æ–¹æ³•å’Œæ•°æ®ï¼Œæ³¨æ˜ä½œè€…å’Œç ”ç©¶æœºæ„ã€‚",
            'general': "è¿™æ˜¯é€šç”¨æœç´¢ç»“æœã€‚è¯·åŸºäºäº‹å®å†…å®¹å›ç­”ï¼Œå¦‚éœ€åˆ›ä½œå¯å‚è€ƒæ­¤å†…å®¹ï¼Œä½†ä¸è¦ç¼–é€ ä¿¡æ¯ã€‚å¯è¿›è¡Œè·¨è¯­è¨€ä¿¡æ¯æ•´åˆã€‚"
        }

        return base_instruction + type_specific.get(content_type, "è¯·å‡†ç¡®æå–ä¿¡æ¯å¹¶ç®€æ´å›ç­”ã€‚")

    def _log_completion(self, start_time, result_count):
        """è®°å½•å®ŒæˆçŠ¶æ€"""
        try:
            elapsed_time = time.time() - start_time
            if result_count > 0:
                status_msg = f"âœ… å®Œæˆ: {result_count}æ¡ç»“æœï¼Œè€—æ—¶ {elapsed_time:.2f}ç§’"
            else:
                status_msg = f"âŒ æœç´¢å¤±è´¥ï¼Œè€—æ—¶ {elapsed_time:.2f}ç§’"

            print(status_msg)
            self.log_signal.emit(status_msg)
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—è®°å½•å¤±è´¥: {e}")


# ä¿æŒå‘åå…¼å®¹
WebKnowledgeEngine = WebSearcher

# æµ‹è¯•ç”¨
if __name__ == "__main__":
    # æµ‹è¯•ä¸¤ç§æœç´¢æ¨¡å¼
    ws = WebSearcher()

    # æµ‹è¯•ç›´æ¥æœç´¢ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
    print("=== æµ‹è¯•ç›´æ¥æœç´¢ï¼ˆç»ˆæé€‚é…ç‰ˆï¼‰ ===")
    result = ws.search("å…´å®‰ç›Ÿå¤©æ°”")
    print(result)

    # æµ‹è¯•ä¿å­˜æœç´¢
    print("\n=== æµ‹è¯•ä¿å­˜æœç´¢ï¼ˆç½‘ç»œçŒäººå¼•æ“ï¼‰ ===")
    saved_files = ws.search_and_save("åŒ—äº¬å¤©æ°”")
    print(f"ä¿å­˜çš„æ–‡ä»¶: {saved_files}")

    # æµ‹è¯•è‹±æ–‡æœç´¢
    print("\n=== æµ‹è¯•è‹±æ–‡æœç´¢ ===")
    result = ws.search("DeepSeek AI assistant features")
    print(result)

    # æµ‹è¯•é˜²å´©æºƒç‰¹æ€§
    print("\n=== æµ‹è¯•é˜²å´©æºƒç‰¹æ€§ ===")
    # æ¨¡æ‹Ÿä¸€ä¸ªä¼šå´©æºƒçš„æŸ¥è¯¢
    ws.search(None)  # è¿™åº”è¯¥ä¸ä¼šå´©æºƒ