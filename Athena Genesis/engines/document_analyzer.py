# -*- coding: utf-8 -*-
"""
æ–‡æ¡£æ™ºèƒ½åˆ†æå™¨ - ä¼ä¸šçº§å®Œæ•´ç‰ˆ + ä¹±ç æ¸…æ´— + åŸå­çº§DNAåˆ†æ (ç»ˆæä¿®å¤å¢å¼ºç‰ˆ)
åŒ…å«ï¼šå…¨é‡å…³é”®è¯(Top100)ã€æ­£åˆ™å¥å¼æŒ–æ˜ã€NLPæƒ…æ„Ÿåˆ†æã€é›·è¾¾å›¾çœŸå®ç®—æ³•ã€æ–‡ä»¶æ„å›¾è¯†åˆ«ã€åŸå­çº§DNAåˆ†æ
æ–°å¢ï¼šæé€Ÿæ‰«ææ¨¡å¼(fast_analyze) + LLMæ·±åº¦åˆ†ææ¨¡å¼
ä¿®å¤ï¼šæ ¸å¿ƒå…³é”®è¯å‡ºç°ä¹±ç ã€ç©ºæ ¼ã€æ§åˆ¶ç¬¦é—®é¢˜
å¢å¼ºï¼šæ•´åˆåŸå­çº§åˆ†æ(è¯çº§ã€å¥çº§ã€è¯æ€§ã€é€»è¾‘å…³è”è¯åˆ†æ)
"""
import re
import jieba
import jieba.analyse
import jieba.posseg as pseg
import collections
import math
import numpy as np
from collections import Counter


class DocumentIntelligenceAnalyzer:
    """
    æ·±åº¦è§£æ„æ–‡æ¡£ï¼Œæå–çµé­‚ç‰¹å¾ + åŸå­çº§DNAåˆ†æ
    æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
    1. deep_analyze: å…¨é‡æ·±åº¦åˆ†æ (è§„åˆ™ç®—æ³•)
    2. fast_analyze: æé€Ÿæ‰«ææ¨¡å¼ (0.01s/æ–‡ä»¶)
    3. llm_analyze: LLMæ·±åº¦è§£è¯» (éœ€æä¾›LLMå¼•æ“)
    """

    def __init__(self, llm_engine=None):
        # ğŸ”¥ æ‰©å±•åœç”¨è¯è¡¨ (æ¥è‡ªæ— æ ‡é¢˜.txt)
        self.stop_words = {
            'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'åœ¨',
            'è¿™', 'é‚£', 'æœ‰', 'ä¸ª', 'ä¹‹', 'ä¸Š', 'ä¸‹', 'æˆ‘ä»¬', 'ä½ ä»¬',
            '1', '2', '3', '4', '5', '6', '7', '8', '9', '0',
            ' ', '\t', '\n', '\r', '\xa0', '\u3000',  # ç©ºç™½å­—ç¬¦
        }

        # ğŸ”¥ æ–°å¢ï¼šåŸå­çº§åˆ†æç»„ä»¶ (æ¥è‡ª3.txt)
        # è¯­æ°”åŠ©è¯åº“ (CR1.2 æƒ…æ„Ÿæƒé‡)
        self.particles = {'å—', 'å‘¢', 'å•Š', 'å‘€', 'å§', 'ç½¢', 'å‘—', 'å˜', 'å“‡'}
        # é€»è¾‘å…³è”è¯ (CR1.3 å¥å­ç»“æ„)
        self.logic_markers = {
            'å› æœ': ['å› ä¸º', 'æ‰€ä»¥', 'å› æ­¤', 'å¯¼è‡´', 'è‡´ä½¿'],
            'è½¬æŠ˜': ['ä½†æ˜¯', 'ç„¶è€Œ', 'ä¸è¿‡', 'å´', 'è™½ç„¶'],
            'é€’è¿›': ['è€Œä¸”', 'å¹¶ä¸”', 'ä¸ä»…', 'ç”šè‡³', 'æ›´'],
            'æ¡ä»¶': ['åªè¦', 'åªæœ‰', 'é™¤é', 'æ— è®º']
        }

        # LLMå¼•æ“ (å¯é€‰ï¼Œç”¨äºæ·±åº¦åˆ†æ)
        self.llm = llm_engine

        # åˆå§‹åŒ–jiebaï¼ŒåŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆå¯é€‰ï¼‰
        try:
            jieba.initialize()
        except:
            pass

    # ==========================================
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ–°å¢ fast_analyze æ–¹æ³• ğŸ”¥ğŸ”¥ğŸ”¥
    # ==========================================
    def fast_analyze(self, content, filename):
        """
        âš¡ [æé€Ÿæ¨¡å¼] å¿«é€Ÿæ‰«æï¼Œä¸æ¶ˆè€— LLM èµ„æº
        é€Ÿåº¦ï¼š0.01s / æ–‡ä»¶
        è¿”å›ç²¾ç®€ç‰ˆåˆ†æç»“æœ
        """
        if not content:
            return self._empty_fast_result(filename)

        # 1. ç‰©ç†ç‰¹å¾
        length = len(content)

        # 2. æå–æ‘˜è¦ (å–å‰ 200 å­— + ç®€å•çš„æ¸…æ´—)
        summary = content[:300].replace('\n', ' ').strip() + "..."

        # 3. æå–å…³é”®è¯ (ä½¿ç”¨ Jieba TF-IDF ç®—æ³•)
        try:
            tags = jieba.analyse.extract_tags(content, topK=8)
            keywords = {tag: 1.0 for tag in tags}
        except:
            keywords = {}

        # 4. ä¼°ç®—é›·è¾¾å›¾æ•°æ® (åŸºäºå…³é”®è¯åŒ¹é…)
        metrics = self._calculate_metrics_rule_based(content)

        return {
            "document_info": {
                "filename": filename,
                "length": length,
                "filepath": filename,  # ç¡®ä¿æœ‰è¿™ä¸ªå­—æ®µ
                "analysis_mode": "fast"
            },
            "semantic_summary": {
                "keywords": keywords,
                "sentiment": "neutral"
            },
            "radar_metrics": metrics,
            "text_report": f"ã€å¿«é€Ÿæ‰«ææ‘˜è¦ã€‘\n{summary}\n\n(åŒå‡»æ–‡æ¡£åˆ—è¡¨å¯è¿›è¡Œæ·±åº¦ AI è§£è¯»)"
        }

    def _calculate_metrics_rule_based(self, text):
        """åŸºäºè§„åˆ™å¿«é€Ÿç”Ÿæˆå…­ç»´æ•°æ® (æ— éœ€ LLM)"""
        base_score = 60
        text_sample = text[:5000] if len(text) > 5000 else text

        # ç®€å•çš„å…³é”®è¯å‘½ä¸­è®¡æ•°
        logic_score = base_score + text_sample.count("å› ä¸º") * 2 + text_sample.count("æ•°æ®") * 2
        emotion_score = base_score + text_sample.count("ï¼") * 5 + text_sample.count("æ„ŸåŠ¨") * 5
        depth_score = base_score + len(text) / 1000  # å­—æ•°è¶Šå¤šè¶Šæ·±

        return {
            "logic": min(95, logic_score),
            "emotion": min(95, emotion_score),
            "creativity": 70,
            "depth": min(95, depth_score),
            "structure": 80,
            "practicality": 80
        }

    # ==========================================
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ LLM æ·±åº¦åˆ†ææ¨¡å¼ ğŸ”¥ğŸ”¥ğŸ”¥
    # ==========================================
    def llm_analyze(self, content, filename):
        """
        [æ·±åº¦æ¨¡å¼] è°ƒç”¨ LLM è¿›è¡Œç²¾è¯»
        éœ€è¦åˆå§‹åŒ–æ—¶ä¼ å…¥ llm_engine
        """
        if not content:
            return self._empty_llm_result(filename)

        # å°è¯•è°ƒç”¨ LLMï¼Œå¦‚æœæ²¡æœ‰ LLM åˆ™è¿”å›è§„åˆ™åˆ†æç»“æœ
        if not self.llm:
            # å¦‚æœæ²¡æœ‰LLMå¼•æ“ï¼Œåˆ™ä½¿ç”¨fast_analyzeä½œä¸ºå…œåº•
            result = self.fast_analyze(content, filename)
            result["document_info"]["analysis_mode"] = "fast (no LLM)"
            result["text_report"] = "LLM æœªè¿æ¥ï¼Œå·²ä½¿ç”¨å¿«é€Ÿæ‰«ææ¨¡å¼"
            return result

        # æ„å»ºLLMæç¤ºè¯
        summary_prompt = f"è¯·é˜…è¯»æ–‡ä»¶ã€Š{filename}ã€‹ï¼Œæç‚¼æ ¸å¿ƒè§‚ç‚¹å’Œæ•°æ®ã€‚\nå†…å®¹ï¼š{content[:5000]}"

        try:
            # è°ƒç”¨LLM
            result_text = self.llm.chat(summary_prompt, options={"temperature": 0.3})
        except Exception as e:
            # LLMè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™åˆ†æ
            print(f"LLMåˆ†æå¤±è´¥: {e}")
            result = self.fast_analyze(content, filename)
            result["document_info"]["analysis_mode"] = "fast (LLM failed)"
            result["text_report"] = f"LLMåˆ†æå¤±è´¥ï¼Œå·²ä½¿ç”¨å¿«é€Ÿæ‰«ææ¨¡å¼\né”™è¯¯ä¿¡æ¯: {str(e)}"
            return result

        # è·å–è§„åˆ™åˆ†æçš„é›·è¾¾å›¾æ•°æ®
        metrics = self._calculate_metrics_rule_based(content)

        # æå–å…³é”®è¯
        try:
            tags = jieba.analyse.extract_tags(content, topK=15)
            keywords = {tag: 1.0 for tag in tags}
        except:
            keywords = {}

        return {
            "document_info": {
                "filename": filename,
                "length": len(content),
                "filepath": filename,
                "analysis_mode": "llm"
            },
            "text_report": result_text,
            "radar_metrics": metrics,
            "semantic_summary": {
                "keywords": keywords,
                "llm_analysis": True
            }
        }

    def deep_analyze(self, content: str, filename: str) -> dict:
        """
        æ‰§è¡Œå…¨æµç¨‹æ·±åº¦åˆ†æ (è§„åˆ™ç®—æ³•ç‰ˆ)
        æ•´åˆäº†ä¹±ç æ¸…æ´—åŠŸèƒ½ + åŸå­çº§DNAåˆ†æ
        """
        if not content:
            return self._empty_result(filename)

        # ğŸ”¥ 1. æ·±åº¦æ¸…æ´—æ–‡æœ¬ (è§£å†³ä¹±ç æ ¸å¿ƒï¼Œæ¥è‡ªæ— æ ‡é¢˜.txt)
        # ä»…ä¿ç•™ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹
        # å»é™¤ä¸å¯è§å­—ç¬¦ã€ç‰¹æ®Šç¬¦å·
        clean_text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ã€\.,!\?:"\'\-\s]', '', content)

        # ç§»é™¤è¿ç»­çš„ç©ºæ ¼å’Œæ¢è¡Œ
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå†…å®¹ï¼Œè¿”å›ç©ºç»“æœ
        if len(clean_text) < 10:
            return self._empty_result(filename)

        total_length = len(clean_text)

        # ğŸ”¥ 2. å…¨é‡å…³é”®è¯æå– (Top 100, å¿…é¡»å…¨é‡ä»¥ä¾›ä»ªè¡¨ç›˜å±•ç¤º)
        # å…è®¸çš„è¯æ€§ï¼šåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€ä¸“å
        allow_pos = ('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'ad', 'an')

        try:
            keywords = jieba.analyse.extract_tags(
                clean_text, topK=100, withWeight=True, allowPOS=allow_pos
            )
        except Exception as e:
            print(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„åˆ†è¯ç»Ÿè®¡
            words = jieba.lcut(clean_text)
            valid_words = [w for w in words if len(w.strip()) > 1 and w not in self.stop_words]
            word_counter = Counter(valid_words)
            keywords = [(word, count) for word, count in word_counter.most_common(50)]

        # ğŸ”¥ å…³é”®è¯æ¸…æ´—ï¼šå»é™¤åœç”¨è¯å’Œä¹±ç 
        keyword_dict = {}
        for k, v in keywords:
            k_str = str(k).strip()
            # è¿‡æ»¤æ¡ä»¶ï¼šä¸åœ¨åœç”¨è¯ä¸­ï¼Œé•¿åº¦>1ï¼Œä»…åŒ…å«æœ‰æ•ˆå­—ç¬¦
            if (k_str not in self.stop_words and
                    len(k_str) > 1 and
                    re.match(r'^[\u4e00-\u9fa5a-zA-Z0-9]+$', k_str)):
                keyword_dict[k_str] = float(v)

        # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–æ–¹æ³•
        if len(keyword_dict) < 5:
            keyword_dict = self._extract_keywords_fallback(clean_text)

        # 3. å¥å¼ç»“æ„æ·±åº¦æŒ–æ˜ (ç”¨äºä»¿å†™å¼•æ“)
        patterns = self._extract_sentence_patterns(clean_text)

        # 4. æ·±åº¦æƒ…æ„Ÿä¸åŸºè°ƒåˆ†æ (ç”Ÿæˆæè¿°æ€§æ–‡æœ¬)
        tone_desc = self._analyze_tone_depth(clean_text)

        # 5. æ„å›¾è¯†åˆ« (è‡ªåŠ¨åˆ¤æ–­æ–‡æ¡£ç±»å‹)
        intent = self._analyze_intent(clean_text, filename)

        # 6. é›·è¾¾å›¾å…­ç»´æŒ‡æ ‡çœŸå®è®¡ç®— (æ ¸å¿ƒç®—æ³•)
        metrics = self._calculate_real_metrics(clean_text, keyword_dict)

        # 7. ğŸ”¥ å¥å­ç»“æ„åˆ†æ (æ¥è‡ªæ— æ ‡é¢˜.txt)
        style_dna = self._analyze_style_dna(clean_text)

        # 8. ğŸ”¥ åŸå­çº§DNAåˆ†æ (æ¥è‡ª3.txt) - æ–°å¢
        atomic_dna = self._analyze_atomic_dna(content)  # ä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡ŒåŸå­çº§åˆ†æ

        # 9. ç”Ÿæˆæ‘˜è¦ (æˆªå–å¼€å¤´+å…³é”®å¥)
        summary = clean_text[:800].replace('\n', ' ') + "..." if len(clean_text) > 800 else clean_text

        # 10. ç»„è£…å®Œæ•´æ•°æ®åŒ…
        return {
            "document_info": {
                "filename": filename,
                "length": total_length,
                "file_type": filename.split('.')[-1].upper() if '.' in filename else "TXT",
                "sentence_count": style_dna.get("sentence_count", 0),
                "analysis_mode": "deep"
            },
            "text_report": summary,
            "intent": intent,
            "semantic_summary": {
                "keywords": keyword_dict,
                "tone": tone_desc,
                "patterns": patterns,
                "sentence_structures": style_dna.get("sentence_structures", {})
            },
            "radar_metrics": metrics,
            "style_dna": style_dna,  # ğŸ”¥ åŸæœ‰é£æ ¼DNA
            "atomic_dna": atomic_dna  # ğŸ”¥ æ–°å¢åŸå­çº§DNAåˆ†æ
        }

    # ==========================================
    # ğŸ”¥ æ–°å¢æ–¹æ³•ï¼šåŸå­çº§DNAåˆ†æ (æ¥è‡ª3.txt)
    # ==========================================
    def _analyze_atomic_dna(self, text: str) -> dict:
        """
        åŸå­çº§DNAåˆ†æ (CR1.1å­—çº§, CR1.2è¯çº§, CR1.3å¥çº§)
        è¿”å›æ·±å±‚å†™ä½œDNAç‰¹å¾
        """
        if not text or len(text.strip()) == 0:
            return self._empty_atomic_dna()

        # åŸå­çº§åˆ†æä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼ˆä¸è¿›è¡Œæ·±åº¦æ¸…æ´—ï¼Œä¿ç•™æ‰€æœ‰å­—ç¬¦ï¼‰
        clean_text = re.sub(r'\s+', '', text)  # ä»…å»é™¤ç©ºç™½å­—ç¬¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', clean_text)
        sentences = [s for s in sentences if len(s) > 1]

        # === Level 1: åŸå­çº§åˆ†æ (è¯æ€§ä¸ç”¨è¯åå¥½) ===
        words_flags = list(pseg.cut(clean_text))  # è¯+è¯æ€§

        # ç»Ÿè®¡è¯æ€§å¯†åº¦ (CR1.2)
        pos_counts = Counter([flag for word, flag in words_flags])
        total_words = len(words_flags)

        # è™šè¯ç‡ (çš„/åœ°/å¾— ä½¿ç”¨ä¹ æƒ¯)
        u_count = pos_counts.get('u', 0)  # åŠ©è¯
        adj_count = pos_counts.get('a', 0)  # å½¢å®¹è¯
        idiom_count = pos_counts.get('i', 0)  # æˆè¯­

        # === Level 2: èŠ‚å¥ä¸å¥æ³• (CR1.3) ===
        sentence_lens = [len(s) for s in sentences]
        if sentence_lens:
            avg_len = np.mean(sentence_lens)
            std_dev = np.std(sentence_lens)  # èŠ‚å¥æ³¢åŠ¨ç‡ (é‡è¦DNA)
            max_len = np.max(sentence_lens)
        else:
            avg_len, std_dev, max_len = 0, 0, 0

        # æ ‡ç‚¹æŒ‡çº¹ (CR1.3 æ ‡ç‚¹åå¥½)
        punct_raw = re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›â€¦â€¦â€”]', text)
        punct_counter = Counter(punct_raw)

        # === Level 3: é€»è¾‘ä¸ä¿®è¾ (CR1.3) ===
        logic_profile = {k: 0 for k in self.logic_markers}
        for type_, keywords in self.logic_markers.items():
            for kw in keywords:
                logic_profile[type_] += text.count(kw)

        # === DNA å»ºæ¨¡ (è®¡ç®—å…­ç»´é›·è¾¾åˆ†) ===
        # 1. é€»è¾‘æ€§ (Logic): å…³è”è¯å¯†åº¦ + å¹³å‡å¥é•¿
        score_logic = min(95, int(logic_profile['å› æœ'] * 5 + logic_profile['è½¬æŠ˜'] * 5 + avg_len * 0.5 + 30))

        # 2. åˆ›é€ åŠ› (Creativity): å½¢å®¹è¯å¯†åº¦ + æˆè¯­å¯†åº¦
        score_creat = min(95, int((adj_count / max(1, total_words)) * 200 + idiom_count * 10 + 40))

        # 3. æƒ…æ„Ÿåº¦ (Emotion): è¯­æ°”è¯ + æ„Ÿå¹å·
        particle_count = sum(text.count(p) for p in self.particles)
        score_emo = min(95, int(particle_count * 5 + punct_counter.get('ï¼', 0) * 8 + 30))

        # 4. ä¸¥è°¨åº¦ (Critical): å¥é•¿æ³¢åŠ¨ä½(ç¨³) + åŠ©è¯å°‘(å¹²ç»ƒ)
        # æ³¢åŠ¨ç‡è¶Šä½ï¼Œç»“æ„è¶Šç¨³ï¼›åŠ©è¯è¶Šå°‘ï¼Œè¶Šåƒå…¬æ–‡
        if total_words > 0:
            score_crit = min(95, int(100 - std_dev + (100 - (u_count / total_words * 500))))
        else:
            score_crit = 50
        if score_crit < 40:
            score_crit = 50

        # 5. ç»“æ„æ„Ÿ (Struct): æ ‡ç‚¹ä¸°å¯Œåº¦ + é€»è¾‘è¯æ€»æ•°
        score_struct = min(95, int(len(punct_counter) * 10 + sum(logic_profile.values()) * 2 + 40))

        # 6. æ·±åº¦ (Depth): ç¯‡å¹… + é•¿éš¾å¥æ¯”ä¾‹
        long_sentence_ratio = sum(1 for l in sentence_lens if l > 40) / max(1, len(sentence_lens))
        score_depth = min(95, int(long_sentence_ratio * 100 + avg_len + 20))

        # æå–é«˜é¢‘å®è¯ (ç”¨äºMimicry)
        keywords = [w for w, f in words_flags if f.startswith('n') or f.startswith('v') or f.startswith('a')]
        vocab_counter = Counter(keywords)

        return {
            "dna_signature": {
                "avg_len": round(avg_len, 1),
                "rhythm_volatility": round(std_dev, 1),  # èŠ‚å¥æ³¢åŠ¨
                "particle_ratio": round(u_count / max(1, total_words), 3) if total_words > 0 else 0,  # è™šè¯ç‡
                "idiom_usage": idiom_count,
                "logic_map": logic_profile,
                "pos_density": dict(pos_counts.most_common(20))  # è¯æ€§å¯†åº¦åˆ†å¸ƒ
            },
            "atomic_radar_metrics": {
                "Logic": score_logic,
                "Creativity": score_creat,
                "Emotion": score_emo,
                "Critical": score_crit,
                "Struct": score_struct,
                "Depth": score_depth
            },
            "atomic_keywords": dict(vocab_counter.most_common(50))
        }

    def _empty_atomic_dna(self):
        """è¿”å›ç©ºçš„åŸå­çº§DNAåˆ†æç»“æœ"""
        return {
            "dna_signature": {
                "avg_len": 0,
                "rhythm_volatility": 0,
                "particle_ratio": 0,
                "idiom_usage": 0,
                "logic_map": {},
                "pos_density": {}
            },
            "atomic_radar_metrics": {
                "Logic": 20, "Creativity": 20, "Emotion": 20,
                "Critical": 20, "Struct": 20, "Depth": 20
            },
            "atomic_keywords": {}
        }

    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šå¥å¼ç»“æ„æŒ–æ˜ (æ¥è‡ª1.txt)
    # ==========================================
    def _extract_sentence_patterns(self, text):
        """
        æ­£åˆ™æŒ–æ˜ç»å…¸å¥å¼ (Few-Shot Prompting ç´ æ)
        """
        # æŒ‰æ ‡ç‚¹æ–­å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text)
        patterns = []

        # è§¦å‘è¯åº“ï¼šè¦†ç›–å…¬æ–‡ã€æ–°é—»ã€å­¦æœ¯ç­‰å¤šç§é£æ ¼
        triggers = [
            "æˆ‘ä»¬è¦", "åšæŒ", "æ¨è¿›", "å¼ºè°ƒ", "æŒ‡å‡º", "æ„å‘³ç€",
            "å¿…é¡»çœ‹åˆ°", "æ€»çš„æ¥çœ‹", "å€¼å¾—æ³¨æ„çš„æ˜¯", "æ ¸å¿ƒåœ¨äº",
            "ä¸ä»…", "æ—¢è¦", "æ˜¯ä»¥", "æ—¨åœ¨", "å›´ç»•"
        ]

        seen = set()

        for s in sentences:
            s = s.strip()
            # é•¿åº¦è¿‡æ»¤ï¼šå¤ªçŸ­æ— æ„ä¹‰ï¼Œå¤ªé•¿LLMå­¦ä¸ä¼š
            if 8 < len(s) < 60:
                for t in triggers:
                    if s.startswith(t):
                        if s not in seen:
                            patterns.append(s)
                            seen.add(s)
                        break

        # è¿”å›å‰15ä¸ªé«˜è´¨é‡å¥å¼
        return list(patterns)[:15]

    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šæ·±åº¦æƒ…æ„Ÿåˆ†æ (æ¥è‡ª1.txt)
    # ==========================================
    def _analyze_tone_depth(self, text):
        """
        åŸºäºè¯è¢‹æ¨¡å‹çš„æ·±åº¦æƒ…æ„Ÿåˆ†æ
        """
        score = 0
        # æ‰©å……æƒ…æ„Ÿè¯å…¸
        pos_words = ["çªç ´", "åˆ›æ–°", "å¢é•¿", "èƒœåˆ©", "è¾‰ç…Œ", "åšæŒ", "ä¼Ÿå¤§", "æ˜¾è‘—", "ä¼˜åŒ–", "æœºé‡", "å®Œå–„", "æå‡"]
        neg_words = ["æŒ‘æˆ˜", "å›°éš¾", "ä¸¥å³»", "é£é™©", "éåˆ¶", "å‹åŠ›", "ä¸è¶³", "çŸ›ç›¾", "æ»å", "å¤æ‚", "ä¸‹æ»‘", "ç“¶é¢ˆ"]

        for w in pos_words:
            score += text.count(w)
        for w in neg_words:
            score -= text.count(w) * 1.2  # è´Ÿé¢è¯æƒé‡ç•¥é«˜

        # åˆ¤å®šåŸºè°ƒ
        base_tone = ""
        if score > 20:
            base_tone = "æ¿€æ˜‚å‘ä¸Šã€å……æ»¡ä¿¡å¿ƒ"
        elif score > 5:
            base_tone = "ç¨³ä¸­æ±‚è¿›ã€å®¢è§‚ç§¯æ"
        elif score > -5:
            base_tone = "å®¡æ…å†·é™ã€ç›´é¢æŒ‘æˆ˜"
        elif score > -20:
            base_tone = "å¿§æ‚£æ„è¯†ã€ä¸¥è‚ƒæ‰¹åˆ¤"
        else:
            base_tone = "å½¢åŠ¿ä¸¥å³»ã€æåº¦æ‚²è§‚"

        return base_tone

    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šæ–‡æ¡£æ„å›¾åˆ†ç±» (æ¥è‡ª1.txt)
    # ==========================================
    def _analyze_intent(self, text, filename):
        """æ–‡æ¡£æ„å›¾åˆ†ç±»"""
        header = text[:300]
        if "é€šçŸ¥" in filename or "å…³äº" in filename:
            return "è¡Œæ”¿é€šçŸ¥"
        if "æŠ¥å‘Š" in filename or "æ€»ç»“" in filename:
            return "å·¥ä½œæ±‡æŠ¥"
        if "æ³•" in filename or "æ¡ä¾‹" in filename:
            return "æ³•å¾‹æ³•è§„"
        if "ç ”ç©¶" in header or "å®éªŒ" in header:
            return "å­¦æœ¯ç ”ç©¶"
        return "é€šç”¨èµ„è®¯"

    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šé›·è¾¾å›¾å…­ç»´æŒ‡æ ‡çœŸå®è®¡ç®— (æ¥è‡ª1.txt)
    # ==========================================
    def _calculate_real_metrics(self, text, keywords):
        """
        é›·è¾¾å›¾å…­ç»´æŒ‡æ ‡çœŸå®è®¡ç®—ç®—æ³•
        """
        L = max(len(text), 1)

        # 1. é€»è¾‘æ€§ (è¿æ¥è¯å¯†åº¦)
        logic_kws = ['å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'é‰´äº', 'ç»¼ä¸Š', 'ä¸€æ–¹é¢', 'åŒæ—¶']
        c_logic = sum(text.count(w) for w in logic_kws)
        score_logic = min(0.95, (c_logic / L) * 1000 / 10)  # å½’ä¸€åŒ–

        # 2. åˆ›é€ åŠ› (æ–°é¢–è¯æ±‡+å…³é”®è¯ç¦»æ•£åº¦)
        create_kws = ['åˆ›æ–°', 'çªç ´', 'é¦–åˆ›', 'æ–°è´¨', 'æ”¹é©', 'å‰æ²¿', 'ç‹¬åˆ›']
        c_create = sum(text.count(w) for w in create_kws)
        diversity = len(keywords) / 100.0
        score_create = min(0.95, (c_create / L * 1000 / 5) * 0.6 + diversity * 0.4)

        # 3. åŒç†å¿ƒ (äººç§°ä»£è¯ä¸æƒ…æ„Ÿè¯)
        empathy_kws = ['æˆ‘ä»¬', 'å¤§å®¶', 'äººæ°‘', 'ç¾¤ä¼—', 'æ„Ÿå—', 'å¿ƒå£°', 'å…³æ€€']
        c_emp = sum(text.count(w) for w in empathy_kws)
        score_emp = min(0.95, (c_emp / L) * 1000 / 8)

        # 4. çŸ¥è¯†å¹¿åº¦ (å…³é”®è¯æƒé‡æ€»å’Œ)
        total_weight = sum(keywords.values())
        score_breadth = min(0.95, total_weight / 25.0)

        # 5. è®°å¿†æ·±åº¦ (å†å²å¼•ç”¨ä¸ç¯‡å¹…)
        depth_kws = ['å†å²', 'å›é¡¾', 'è¿‡å»', 'ä»¥æ¥', 'ç™¾å¹´', 'æ ¹æº']
        c_depth = sum(text.count(w) for w in depth_kws)
        score_depth = min(0.95, (c_depth / L * 1000 / 5) * 0.3 + min(1.0, L / 5000.0) * 0.7)

        # 6. æ‰§è¡ŒåŠ› (åŠ¨è¯å¯†åº¦)
        action_kws = ['è½å®', 'æ‰§è¡Œ', 'å®æ–½', 'æ¨è¿›', 'å®Œæˆ', 'ç¡®ä¿', 'è¡ŒåŠ¨', 'æ‰“èµ¢']
        c_act = sum(text.count(w) for w in action_kws)
        score_exec = min(0.95, (c_act / L) * 1000 / 10)

        # ä¿åº•ä¿®æ­£ (é˜²æ­¢é›·è¾¾å›¾ç¼©æˆä¸€ç‚¹)
        def clamp(v):
            return max(0.25, v)

        return {
            "é€»è¾‘æ€§": clamp(score_logic),
            "åˆ›é€ åŠ›": clamp(score_create),
            "åŒç†å¿ƒ": clamp(score_emp),
            "çŸ¥è¯†å¹¿åº¦": clamp(score_breadth),
            "è®°å¿†æ·±åº¦": clamp(score_depth),
            "æ‰§è¡ŒåŠ›": clamp(score_exec)
        }

    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šé£æ ¼DNAåˆ†æ (æ¥è‡ªæ— æ ‡é¢˜.txt)
    # ==========================================
    def _analyze_style_dna(self, text):
        """
        åˆ†ææ–‡æ¡£çš„é£æ ¼DNA
        åŒ…æ‹¬ï¼šå¹³å‡å¥é•¿ã€æ ‡ç‚¹ä½¿ç”¨ã€å¥å­ç»“æ„ç­‰
        """
        if not text:
            return {
                "avg_sentence_length": 0,
                "sentence_volatility": 0,
                "punctuation_profile": {},
                "sentence_count": 0,
                "sentence_structures": {}
            }

        # 1. æ ‡ç‚¹ç¬¦å·æŒ‡çº¹
        punctuations = re.findall(r'[ï¼ã€‚ï¼Ÿï¼Œã€ï¼šï¼›â€¦â€¦]', text)
        punct_counter = Counter(punctuations)

        # 2. å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 5]  # å¿½ç•¥å¤ªçŸ­çš„ç¢ç‰‡

        sentence_count = len(sentences)

        # 3. å¥å­é•¿åº¦åˆ†æ
        if sentences:
            lens = [len(s) for s in sentences]
            avg_len = np.mean(lens) if lens else 0
            std_dev = np.std(lens) if len(lens) > 1 else 0
        else:
            avg_len = 0
            std_dev = 0

        # 4. å¥å­ç»“æ„åˆ†æï¼ˆå¥å¼å¼€å¤´è¯ï¼‰
        sentence_structures = Counter()
        for s in sentences:
            # è·å–å‰2ä¸ªå­—ç¬¦ä½œä¸ºå¥å¼ç‰¹å¾
            if len(s) >= 2:
                prefix = s[:2]
                if re.match(r'^[\u4e00-\u9fa5]+$', prefix):  # ç¡®ä¿æ˜¯ä¸­æ–‡å­—ç¬¦
                    sentence_structures[prefix] += 1

        return {
            "avg_sentence_length": round(avg_len, 1),
            "sentence_volatility": round(std_dev, 1),
            "punctuation_profile": dict(punct_counter.most_common(10)),
            "sentence_count": sentence_count,
            "sentence_structures": dict(sentence_structures.most_common(10))
        }

    # ==========================================
    # ğŸ”¥ å¤‡ç”¨æ–¹æ³•ï¼šå¤‡ç”¨å…³é”®è¯æå–
    # ==========================================
    def _extract_keywords_fallback(self, text):
        """
        å¤‡ç”¨å…³é”®è¯æå–æ–¹æ³•
        å½“ä¸»æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨
        """
        # åŸºç¡€åˆ†è¯
        words = jieba.lcut(text)

        # è¿‡æ»¤ï¼šå¿…é¡»é•¿åº¦>1ï¼Œä¸”ä¸åœ¨åœç”¨è¯ä¸­
        valid_words = [w for w in words if len(w.strip()) > 1 and w not in self.stop_words]

        # ç»Ÿè®¡è¯é¢‘
        word_counter = Counter(valid_words)

        # å–å‰50ä¸ªé«˜é¢‘è¯
        top_words = word_counter.most_common(50)

        # è½¬æ¢ä¸ºæƒé‡æ ¼å¼ï¼ˆä½¿ç”¨è¯é¢‘çš„logä½œä¸ºæƒé‡ï¼‰
        keyword_dict = {}
        if top_words:
            max_freq = top_words[0][1]
            for word, freq in top_words:
                # å½’ä¸€åŒ–æƒé‡ (0.1-1.0)
                weight = 0.1 + 0.9 * (freq / max_freq) if max_freq > 0 else 0.1
                keyword_dict[word] = round(weight, 2)

        return keyword_dict

    # ==========================================
    # ğŸ”¥ ç®€åŒ–ç‰ˆåˆ†ææ¥å£
    # ==========================================
    def analyze(self, content: str) -> dict:
        """
        ç®€åŒ–ç‰ˆåˆ†ææ¥å£ï¼ˆå…¼å®¹æ— æ ‡é¢˜.txtçš„æ¥å£ï¼‰
        é»˜è®¤ä½¿ç”¨æ·±åº¦åˆ†ææ¨¡å¼
        """
        return self.deep_analyze(content, "unknown.txt")

    # ==========================================
    # ğŸ”¥ ç©ºç»“æœç”Ÿæˆå™¨
    # ==========================================
    def _empty_result(self, filename="unknown.txt"):
        """è¿”å›ç©ºç»“æœï¼ˆæ·±åº¦åˆ†ææ¨¡å¼ï¼‰"""
        return {
            "document_info": {
                "filename": filename,
                "length": 0,
                "file_type": "UNKNOWN",
                "sentence_count": 0,
                "analysis_mode": "deep"
            },
            "text_report": "",
            "intent": "æœªçŸ¥ç±»å‹",
            "semantic_summary": {
                "keywords": {},
                "tone": "ä¸­æ€§",
                "patterns": [],
                "sentence_structures": {}
            },
            "radar_metrics": {
                "é€»è¾‘æ€§": 0.5,
                "åˆ›é€ åŠ›": 0.5,
                "åŒç†å¿ƒ": 0.5,
                "çŸ¥è¯†å¹¿åº¦": 0.5,
                "è®°å¿†æ·±åº¦": 0.5,
                "æ‰§è¡ŒåŠ›": 0.5
            },
            "style_dna": {
                "avg_sentence_length": 0,
                "sentence_volatility": 0,
                "punctuation_profile": {},
                "sentence_count": 0,
                "sentence_structures": {}
            },
            "atomic_dna": self._empty_atomic_dna()  # ğŸ”¥ æ–°å¢åŸå­çº§DNAç©ºç»“æœ
        }

    def _empty_fast_result(self, filename="unknown.txt"):
        """è¿”å›ç©ºç»“æœï¼ˆå¿«é€Ÿåˆ†ææ¨¡å¼ï¼‰"""
        return {
            "document_info": {
                "filename": filename,
                "length": 0,
                "filepath": filename,
                "analysis_mode": "fast"
            },
            "semantic_summary": {
                "keywords": {},
                "sentiment": "neutral"
            },
            "radar_metrics": {
                "logic": 60,
                "emotion": 60,
                "creativity": 70,
                "depth": 60,
                "structure": 80,
                "practicality": 80
            },
            "text_report": "æ–‡æ¡£ä¸ºç©ºæˆ–æ— æ³•åˆ†æ"
        }

    def _empty_llm_result(self, filename="unknown.txt"):
        """è¿”å›ç©ºç»“æœï¼ˆLLMåˆ†ææ¨¡å¼ï¼‰"""
        return {
            "document_info": {
                "filename": filename,
                "length": 0,
                "filepath": filename,
                "analysis_mode": "llm"
            },
            "text_report": "æ–‡æ¡£ä¸ºç©ºæˆ–æ— æ³•åˆ†æ",
            "radar_metrics": {
                "logic": 60,
                "emotion": 60,
                "creativity": 70,
                "depth": 60,
                "structure": 80,
                "practicality": 80
            },
            "semantic_summary": {
                "keywords": {},
                "llm_analysis": False
            }
        }

    # ==========================================
    # ğŸ”¥ æ‰¹é‡åˆ†æ
    # ==========================================
    def batch_analyze(self, contents: list, filenames: list, mode="deep") -> list:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªæ–‡æ¡£
        mode: "deep" | "fast" | "llm"
        """
        results = []
        for content, filename in zip(contents, filenames):
            try:
                if mode == "fast":
                    result = self.fast_analyze(content, filename)
                elif mode == "llm":
                    result = self.llm_analyze(content, filename)
                else:  # deep
                    result = self.deep_analyze(content, filename)
                results.append(result)
            except Exception as e:
                print(f"åˆ†ææ–‡æ¡£ {filename} æ—¶å‡ºé”™: {e}")
                if mode == "fast":
                    results.append(self._empty_fast_result(filename))
                elif mode == "llm":
                    results.append(self._empty_llm_result(filename))
                else:
                    results.append(self._empty_result(filename))

        return results

    # ==========================================
    # ğŸ”¥ æ–‡æœ¬æ¸…æ´—æ¥å£
    # ==========================================
    def clean_text(self, content: str) -> str:
        """
        å¯¹å¤–æä¾›çš„æ–‡æœ¬æ¸…æ´—æ¥å£
        """
        if not content:
            return ""

        # æ·±åº¦æ¸…æ´—æ–‡æœ¬
        clean_text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ã€\.,!\?:"\'\-\s]', '', content)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        return clean_text

    # ==========================================
    # ğŸ”¥ è¯Šæ–­æ¨¡å¼
    # ==========================================
    def diagnose_analysis(self, content: str, filename: str) -> dict:
        """
        è¯Šæ–­æ¨¡å¼ï¼šè¿”å›åˆ†æè¿‡ç¨‹ä¸­çš„ä¸­é—´ç»“æœ
        ç”¨äºè°ƒè¯•å’Œä¼˜åŒ–
        """
        # åŸå§‹æ–‡æœ¬ä¿¡æ¯
        original_length = len(content) if content else 0

        # æ¸…æ´—åçš„æ–‡æœ¬
        clean_text = self.clean_text(content)
        clean_length = len(clean_text)

        # åˆ†è¯æµ‹è¯•
        words = jieba.lcut(clean_text) if clean_text else []
        word_count = len(words)

        # å…³é”®è¯æå–æµ‹è¯•
        try:
            test_keywords = jieba.analyse.extract_tags(
                clean_text, topK=20, withWeight=False
            ) if clean_text else []
        except:
            test_keywords = []

        return {
            "diagnostics": {
                "original_length": original_length,
                "clean_length": clean_length,
                "word_count": word_count,
                "clean_text_sample": clean_text[:200] + "..." if clean_text else "",
                "test_keywords": test_keywords[:10],
                "stop_words_count": len(self.stop_words),
                "atomic_components": {
                    "particles": len(self.particles),
                    "logic_markers": len(self.logic_markers)
                },
                "llm_available": self.llm is not None
            },
            "full_analysis": self.deep_analyze(content, filename) if content else self._empty_result(filename)
        }